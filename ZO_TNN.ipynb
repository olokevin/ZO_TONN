{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N9HKwdDrYXAP"
   },
   "source": [
    "# **Load Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12568,
     "status": "ok",
     "timestamp": 1671402295079,
     "user": {
      "displayName": "赵业权",
      "userId": "15926883842834523025"
     },
     "user_tz": 480
    },
    "id": "vF4q9gu8XvFa",
    "outputId": "bb249f6e-5a96-41a2-b906-7808b06dae5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([128, 1, 28, 28])\n",
      "Shape of y: torch.Size([128]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "# add path\n",
    "import os,sys; \n",
    "sys.path.append('./tensor_layers')\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "# Load Dataset\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, f1_score\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from utils import train,test,get_net,Net,OrthoTONN\n",
    "import math\n",
    "import time\n",
    "\n",
    "# Enable this and writer ovject to track metrics during training\n",
    "from torch.utils.tensorboard import SummaryWriter \n",
    "\n",
    "DTYPE = torch.FloatTensor\n",
    "LONG = torch.LongTensor\n",
    "\n",
    "\n",
    "# Training parameters\n",
    "epochs = 20\n",
    "model_path_dir = './tt_models/'\n",
    "\n",
    "patience = 10\n",
    "\n",
    "factor_lr = 0.003\n",
    "lr = 0.001\n",
    "\n",
    "r = 8\n",
    "batch_sz = 128\n",
    "decay = 0.001\n",
    "cuda = 1\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "dtype = torch.float32\n",
    "\n",
    "complete = True\n",
    "curr_patience = patience\n",
    "\n",
    "# Download training data from open datasets.\n",
    "training_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "# Download test data from open datasets.\n",
    "test_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_sz)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_sz)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "i491uYQXZ9Sw"
   },
   "source": [
    "# Test Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "RhPI4yzgLXal"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "[array([[1., 1.],\n",
      "       [1., 1.]])]\n"
     ]
    }
   ],
   "source": [
    "G = []\n",
    "a = 2\n",
    "G.append(a)\n",
    "b = np.ones([2,2])\n",
    "G.append(b)\n",
    "t = G.pop(0)\n",
    "print(t)\n",
    "print(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def show_images(imgs, num_rows, num_cols, titles=None, scale=1.5):  #@save\n",
    "    \"\"\"绘制图像列表\"\"\"\n",
    "    figsize = (num_cols * scale, num_rows * scale)\n",
    "    _, axes = plt.subplots(num_rows, num_cols, figsize=figsize)\n",
    "    axes = axes.flatten()\n",
    "    for i, (ax, img) in enumerate(zip(axes, imgs)):\n",
    "        if torch.is_tensor(img):\n",
    "            # 图片张量\n",
    "            ax.imshow(img.numpy())\n",
    "        else:\n",
    "            # PIL图片\n",
    "            ax.imshow(img)\n",
    "        ax.axes.get_xaxis().set_visible(False)\n",
    "        ax.axes.get_yaxis().set_visible(False)\n",
    "        if titles:\n",
    "            ax.set_title(titles[i])\n",
    "    return axes\n",
    "\n",
    "def get_fashion_mnist_labels(labels):  #@save\n",
    "    \"\"\"返回Fashion-MNIST数据集的文本标签\"\"\"\n",
    "    text_labels = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat',\n",
    "                   'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']\n",
    "    return [text_labels[int(i)] for i in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = next(iter(DataLoader(training_data, batch_size=18)))\n",
    "show_images(X.reshape(18, 28, 28), 2, 9, titles=get_fashion_mnist_labels(y))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "JNBkXWefLXan"
   },
   "source": [
    "## ZOO training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open tensorboard\n",
    "%load_ext tensorboard\n",
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "my_dir = \"tt_runs/.../\" + now.strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n",
    "%tensorboard --logdir = my_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrthoTONN(\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (fc1): TensorizedLinear(\n",
       "    in_features=784, out_features=1024, bias=False\n",
       "    (tensor): TensorTrainMatrix(\n",
       "      (trainable_variables): ParameterList(\n",
       "          (0): Parameter containing: [torch.FloatTensor of size 1x4x4x20]\n",
       "          (1): Parameter containing: [torch.FloatTensor of size 20x7x8x20]\n",
       "          (2): Parameter containing: [torch.FloatTensor of size 20x7x8x20]\n",
       "          (3): Parameter containing: [torch.FloatTensor of size 20x4x4x1]\n",
       "          (4): Parameter containing: [torch.FloatTensor of size 1x4x4x20]\n",
       "          (5): Parameter containing: [torch.FloatTensor of size 20x7x8x20]\n",
       "          (6): Parameter containing: [torch.FloatTensor of size 20x7x8x20]\n",
       "          (7): Parameter containing: [torch.FloatTensor of size 20x4x4x1]\n",
       "          (8): Parameter containing: [torch.FloatTensor of size 20]\n",
       "          (9): Parameter containing: [torch.FloatTensor of size 20]\n",
       "          (10): Parameter containing: [torch.FloatTensor of size 20]\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc2): TensorizedLinear(\n",
       "    in_features=1024, out_features=10, bias=False\n",
       "    (tensor): TensorTrainMatrix(\n",
       "      (trainable_variables): ParameterList(\n",
       "          (0): Parameter containing: [torch.FloatTensor of size 1x4x1x20]\n",
       "          (1): Parameter containing: [torch.FloatTensor of size 20x8x5x20]\n",
       "          (2): Parameter containing: [torch.FloatTensor of size 20x8x2x20]\n",
       "          (3): Parameter containing: [torch.FloatTensor of size 20x4x1x1]\n",
       "          (4): Parameter containing: [torch.FloatTensor of size 1x4x1x20]\n",
       "          (5): Parameter containing: [torch.FloatTensor of size 20x8x5x20]\n",
       "          (6): Parameter containing: [torch.FloatTensor of size 20x8x2x20]\n",
       "          (7): Parameter containing: [torch.FloatTensor of size 20x4x1x1]\n",
       "          (8): Parameter containing: [torch.FloatTensor of size 20]\n",
       "          (9): Parameter containing: [torch.FloatTensor of size 20]\n",
       "          (10): Parameter containing: [torch.FloatTensor of size 20]\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = OrthoTONN(tensor_type='TensorTrainMatrix',max_rank=20,dropouts=0.5,prior_type='log_uniform',eta=1.0, device=device,dtype=dtype)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "0 not perturbed tensor([[[-0.0300,  0.1934,  0.0946, -0.2166,  0.0571,  0.0158, -0.0501,\n",
      "          -0.1083, -0.0404,  0.1597, -0.2034,  0.0215,  0.1386, -0.3950,\n",
      "          -0.3807, -0.1640,  0.0340,  0.2436, -0.1513,  0.0595],\n",
      "         [ 0.1101, -0.0522,  0.0640, -0.1077, -0.1011,  0.0351,  0.0347,\n",
      "           0.0011,  0.2319,  0.0019, -0.1474,  0.1417, -0.2221, -0.0791,\n",
      "           0.2264, -0.1344, -0.1141,  0.0501, -0.1734, -0.0954],\n",
      "         [ 0.0189, -0.2531, -0.0156, -0.2341, -0.0976, -0.0413, -0.1473,\n",
      "           0.1580, -0.1274,  0.1829, -0.0721, -0.2650,  0.0130, -0.0501,\n",
      "          -0.2425, -0.0843, -0.2100,  0.0079,  0.0408,  0.0763],\n",
      "         [ 0.1462, -0.2876, -0.0197, -0.2936, -0.4483,  0.1269,  0.2732,\n",
      "           0.1828, -0.0994, -0.0145,  0.3505, -0.1666, -0.0502, -0.0100,\n",
      "          -0.2068, -0.0680, -0.1466, -0.0960, -0.0993, -0.1043]],\n",
      "\n",
      "        [[-0.0949,  0.3634, -0.0135,  0.1492,  0.0739,  0.0257,  0.2212,\n",
      "           0.1857, -0.2324,  0.0070, -0.1968,  0.1711, -0.2666, -0.1101,\n",
      "          -0.2347,  0.1981,  0.0664, -0.3293, -0.0885,  0.1229],\n",
      "         [ 0.0032, -0.0694, -0.0364, -0.1354,  0.0294,  0.0848,  0.0660,\n",
      "           0.0598, -0.1384, -0.1105, -0.2383, -0.0779,  0.1389,  0.0724,\n",
      "          -0.0051, -0.2772,  0.1817,  0.2186,  0.1203, -0.3441],\n",
      "         [-0.2050,  0.0251,  0.0806, -0.2747, -0.2320, -0.0341, -0.0498,\n",
      "          -0.1221, -0.0676, -0.0808,  0.2169, -0.0871,  0.2765, -0.0102,\n",
      "          -0.0252, -0.0053, -0.0514, -0.1104, -0.0646, -0.2026],\n",
      "         [ 0.0401,  0.0389,  0.1175, -0.2873, -0.2849,  0.0040,  0.1173,\n",
      "          -0.1004, -0.2633,  0.1199,  0.0757,  0.2245, -0.0258,  0.0035,\n",
      "          -0.2820, -0.0959,  0.1132, -0.2042,  0.1704, -0.1057]],\n",
      "\n",
      "        [[-0.2878,  0.0655, -0.1453, -0.0017, -0.1353,  0.2157,  0.2957,\n",
      "           0.0685, -0.2716,  0.2635, -0.0691,  0.0087, -0.1055, -0.0837,\n",
      "          -0.0828,  0.3215,  0.4872, -0.1573, -0.3069,  0.0012],\n",
      "         [ 0.0373, -0.0656, -0.1357,  0.3116,  0.0711, -0.0091,  0.1034,\n",
      "           0.0080, -0.4977,  0.0983,  0.1579,  0.1172, -0.1110,  0.1058,\n",
      "           0.3101, -0.2142, -0.0853,  0.1472,  0.1246, -0.0990],\n",
      "         [ 0.3351, -0.0854, -0.2199, -0.1896,  0.1563,  0.0759,  0.0072,\n",
      "          -0.1609, -0.0725, -0.1717,  0.2577,  0.0723,  0.0293, -0.2007,\n",
      "           0.0008,  0.0396,  0.2416, -0.0087, -0.0607, -0.0531],\n",
      "         [-0.1791,  0.2147, -0.4778,  0.0378,  0.0562,  0.1214,  0.0233,\n",
      "           0.0642, -0.1581, -0.0326,  0.1190, -0.0894, -0.1529, -0.0250,\n",
      "           0.1477, -0.0315,  0.0357, -0.0051, -0.1206, -0.0273]],\n",
      "\n",
      "        [[ 0.1184,  0.0920,  0.0738, -0.1104,  0.0842,  0.2028, -0.0378,\n",
      "          -0.1823,  0.0225, -0.0853,  0.2822,  0.1426,  0.0273,  0.0680,\n",
      "           0.0613, -0.0025, -0.1619,  0.3354,  0.2211, -0.1887],\n",
      "         [-0.1402, -0.2581, -0.1682,  0.0073,  0.2453, -0.2889,  0.1495,\n",
      "           0.1587, -0.0366, -0.0883, -0.1064,  0.0564,  0.1463, -0.0863,\n",
      "           0.0564,  0.0054,  0.0005,  0.0295, -0.0760,  0.1169],\n",
      "         [ 0.0637,  0.0438, -0.2412, -0.1516, -0.0814, -0.1512,  0.1906,\n",
      "           0.0386,  0.1374, -0.1994,  0.0783,  0.1614,  0.1851, -0.1409,\n",
      "          -0.0173, -0.0446,  0.0777, -0.1226, -0.0417, -0.0735],\n",
      "         [ 0.0453,  0.0383,  0.2596, -0.0549, -0.1825, -0.2417,  0.2473,\n",
      "           0.1343,  0.1844, -0.0979,  0.0631, -0.3804,  0.0313,  0.0371,\n",
      "           0.2080, -0.2212, -0.0573, -0.0393,  0.3093, -0.0335]]])\n",
      "0 perturbed tensor([[[-0.0419,  0.2042,  0.0747, -0.2107,  0.0692,  0.0218, -0.0387,\n",
      "          -0.1070, -0.0326,  0.1447, -0.1959,  0.0068,  0.1431, -0.3929,\n",
      "          -0.3646, -0.1589,  0.0336,  0.2410, -0.1623,  0.0563],\n",
      "         [ 0.1033, -0.0513,  0.0635, -0.1187, -0.1084,  0.0230,  0.0299,\n",
      "           0.0081,  0.2137,  0.0275, -0.1494,  0.1433, -0.2275, -0.0595,\n",
      "           0.2128, -0.1244, -0.1095,  0.0506, -0.1772, -0.0822],\n",
      "         [ 0.0126, -0.2587, -0.0146, -0.2424, -0.0909, -0.0338, -0.1551,\n",
      "           0.1610, -0.1147,  0.1814, -0.0728, -0.2794,  0.0182, -0.0630,\n",
      "          -0.2514, -0.0977, -0.2040,  0.0019,  0.0359,  0.0585],\n",
      "         [ 0.1469, -0.2863, -0.0152, -0.2954, -0.4552,  0.1264,  0.2593,\n",
      "           0.1902, -0.1003, -0.0239,  0.3545, -0.1869, -0.0371, -0.0126,\n",
      "          -0.2150, -0.0643, -0.1367, -0.0850, -0.0780, -0.1069]],\n",
      "\n",
      "        [[-0.0922,  0.3595,  0.0111,  0.1450,  0.0619,  0.0237,  0.2150,\n",
      "           0.1931, -0.2380, -0.0149, -0.1872,  0.1676, -0.2811, -0.1080,\n",
      "          -0.2440,  0.1891,  0.0558, -0.3206, -0.0793,  0.1234],\n",
      "         [ 0.0021, -0.0789, -0.0402, -0.1253,  0.0132,  0.0903,  0.0587,\n",
      "           0.0703, -0.1387, -0.1025, -0.2419, -0.0851,  0.1435,  0.0588,\n",
      "          -0.0026, -0.2778,  0.1652,  0.2049,  0.1167, -0.3534],\n",
      "         [-0.1995,  0.0162,  0.0818, -0.2842, -0.2298, -0.0288, -0.0673,\n",
      "          -0.0944, -0.0659, -0.0910,  0.2275, -0.0711,  0.2719, -0.0048,\n",
      "          -0.0267, -0.0141, -0.0390, -0.1072, -0.0609, -0.2158],\n",
      "         [ 0.0463,  0.0431,  0.1215, -0.2832, -0.2710, -0.0048,  0.1257,\n",
      "          -0.1075, -0.2649,  0.1415,  0.0512,  0.2266, -0.0227,  0.0140,\n",
      "          -0.2942, -0.1046,  0.1188, -0.1970,  0.1720, -0.1095]],\n",
      "\n",
      "        [[-0.2947,  0.0595, -0.1528,  0.0058, -0.1264,  0.2170,  0.2967,\n",
      "           0.0879, -0.2831,  0.2593, -0.0640,  0.0079, -0.0988, -0.0784,\n",
      "          -0.0818,  0.3321,  0.4813, -0.1621, -0.3037,  0.0178],\n",
      "         [ 0.0363, -0.0520, -0.1366,  0.2965,  0.0771, -0.0088,  0.0942,\n",
      "          -0.0089, -0.5090,  0.0916,  0.1560,  0.1246, -0.1119,  0.1027,\n",
      "           0.3060, -0.2102, -0.0806,  0.1484,  0.1329, -0.1109],\n",
      "         [ 0.3408, -0.0684, -0.2315, -0.1997,  0.1554,  0.0936,  0.0078,\n",
      "          -0.1593, -0.0758, -0.1818,  0.2552,  0.0777,  0.0285, -0.2130,\n",
      "           0.0022,  0.0183,  0.2344, -0.0182, -0.0693, -0.0682],\n",
      "         [-0.1881,  0.1938, -0.4834,  0.0371,  0.0504,  0.0985,  0.0278,\n",
      "           0.0540, -0.1325, -0.0162,  0.1341, -0.0761, -0.1557, -0.0255,\n",
      "           0.1532, -0.0334,  0.0120, -0.0051, -0.1074, -0.0197]],\n",
      "\n",
      "        [[ 0.1182,  0.0903,  0.0951, -0.1115,  0.0906,  0.1993, -0.0331,\n",
      "          -0.1996,  0.0314, -0.0931,  0.2891,  0.1505,  0.0337,  0.0767,\n",
      "           0.0658, -0.0021, -0.1792,  0.3448,  0.2176, -0.2017],\n",
      "         [-0.1557, -0.2608, -0.1882,  0.0111,  0.2448, -0.2783,  0.1405,\n",
      "           0.1517, -0.0312, -0.0942, -0.0986,  0.0591,  0.1549, -0.0950,\n",
      "           0.0442,  0.0110, -0.0100,  0.0295, -0.0788,  0.1164],\n",
      "         [ 0.0627,  0.0653, -0.2422, -0.1367, -0.0950, -0.1368,  0.1916,\n",
      "           0.0334,  0.1358, -0.2184,  0.0830,  0.1444,  0.1925, -0.1412,\n",
      "          -0.0183, -0.0334,  0.0909, -0.1243, -0.0360, -0.0790],\n",
      "         [ 0.0435,  0.0396,  0.2497, -0.0735, -0.1715, -0.2446,  0.2691,\n",
      "           0.1318,  0.1882, -0.1011,  0.0935, -0.3694,  0.0462,  0.0461,\n",
      "           0.2082, -0.2115, -0.0586, -0.0332,  0.2937, -0.0191]]])\n",
      "1 not perturbed tensor([[[ 2.5688e-01,  1.6694e-01, -2.0172e-02,  ..., -1.8535e-01,\n",
      "          -7.6712e-02, -8.3315e-02],\n",
      "         [ 9.4065e-02,  1.2914e-01,  1.7826e-01,  ...,  4.3093e-02,\n",
      "          -4.7715e-02,  7.9962e-02],\n",
      "         [ 8.4661e-02,  1.1113e-01,  6.7286e-02,  ..., -1.4310e-01,\n",
      "           1.7444e-02, -1.3919e-01],\n",
      "         ...,\n",
      "         [ 1.1955e-01, -1.0709e-01, -1.4580e-01,  ...,  1.0490e-01,\n",
      "           3.2788e-02,  4.0320e-02],\n",
      "         [-1.4156e-01,  1.0145e-01, -7.8359e-02,  ..., -3.0549e-01,\n",
      "          -1.9093e-01,  5.1589e-02],\n",
      "         [ 2.2903e-01, -9.8430e-02,  1.8477e-01,  ...,  1.6245e-01,\n",
      "           1.0354e-01,  2.4528e-01]],\n",
      "\n",
      "        [[ 1.0558e-01,  3.3003e-01, -3.0254e-01,  ...,  1.2128e-01,\n",
      "           3.1428e-01, -2.1750e-01],\n",
      "         [-1.0990e-01,  1.8622e-01, -2.9110e-02,  ...,  2.8718e-01,\n",
      "          -2.0484e-02, -2.3368e-01],\n",
      "         [ 1.9315e-01, -7.5104e-02,  9.4569e-02,  ..., -7.7223e-02,\n",
      "          -1.2719e-01, -3.1079e-02],\n",
      "         ...,\n",
      "         [-3.1105e-01, -5.6264e-02, -8.9641e-03,  ...,  3.4036e-01,\n",
      "           5.0120e-02,  2.3724e-01],\n",
      "         [ 2.2243e-01,  1.5454e-01,  7.3892e-02,  ...,  5.1957e-02,\n",
      "           9.4796e-02,  9.8108e-02],\n",
      "         [ 9.9649e-04, -2.0306e-01,  1.2666e-01,  ..., -5.7813e-02,\n",
      "           3.5434e-02, -1.1305e-01]],\n",
      "\n",
      "        [[-2.0820e-01,  1.6929e-01, -2.8791e-02,  ...,  2.3183e-02,\n",
      "          -1.2263e-01, -6.1354e-02],\n",
      "         [-2.4546e-01, -2.1261e-01, -5.2204e-02,  ..., -3.8320e-01,\n",
      "          -6.8834e-02,  7.7998e-02],\n",
      "         [ 8.0013e-02, -6.7737e-02, -5.8743e-02,  ...,  6.8237e-02,\n",
      "          -6.6814e-02,  3.3626e-02],\n",
      "         ...,\n",
      "         [ 5.9990e-02,  1.6690e-02,  1.3868e-01,  ...,  1.0295e-01,\n",
      "          -9.6333e-02, -4.9981e-02],\n",
      "         [ 1.1663e-01,  1.0438e-01,  1.4446e-02,  ..., -1.1087e-01,\n",
      "           8.1024e-02, -2.1859e-01],\n",
      "         [-1.3459e-01, -1.4012e-01,  8.1604e-02,  ..., -1.0603e-01,\n",
      "           7.8429e-02, -2.1364e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.2091e-01, -5.5870e-02, -2.0678e-01,  ..., -5.1991e-02,\n",
      "           6.5755e-02, -4.7138e-02],\n",
      "         [-1.3067e-01,  1.3875e-01,  1.7950e-01,  ..., -2.2980e-01,\n",
      "           5.9399e-02, -5.4700e-02],\n",
      "         [-2.0731e-01, -8.0414e-02,  1.6754e-01,  ...,  1.3274e-02,\n",
      "           3.6115e-02,  4.6237e-01],\n",
      "         ...,\n",
      "         [ 3.0748e-02, -2.1016e-01, -1.8926e-01,  ..., -2.1226e-02,\n",
      "           1.6564e-01, -9.7293e-02],\n",
      "         [-1.5239e-01, -4.8505e-02,  1.2638e-01,  ...,  1.0792e-02,\n",
      "          -1.5030e-01,  2.1994e-01],\n",
      "         [-2.1470e-01, -3.1771e-01,  1.6991e-01,  ...,  1.0367e-01,\n",
      "          -2.4348e-01,  7.7052e-02]],\n",
      "\n",
      "        [[-2.0016e-02, -3.8262e-02,  1.3511e-02,  ..., -1.9710e-01,\n",
      "           1.4196e-01, -2.4140e-01],\n",
      "         [-2.6677e-01,  2.9408e-01, -1.9744e-02,  ..., -1.1886e-01,\n",
      "           2.1163e-01, -2.3453e-02],\n",
      "         [-1.9573e-02,  1.0931e-01,  1.2534e-01,  ...,  1.7057e-02,\n",
      "          -1.0649e-01, -8.2342e-02],\n",
      "         ...,\n",
      "         [ 1.0211e-02,  3.7881e-02,  2.2315e-01,  ...,  3.5148e-02,\n",
      "           7.6176e-02,  3.7901e-02],\n",
      "         [-1.9647e-01, -1.6987e-01,  3.0498e-01,  ...,  1.2102e-01,\n",
      "          -1.3163e-01, -7.2782e-02],\n",
      "         [-5.8104e-02, -5.4298e-02, -1.8292e-02,  ..., -9.3334e-02,\n",
      "          -2.4204e-01, -1.9136e-01]],\n",
      "\n",
      "        [[-9.9633e-02, -1.0422e-01,  2.4890e-02,  ...,  1.3125e-01,\n",
      "          -2.2241e-02, -6.2747e-03],\n",
      "         [ 3.3494e-01,  2.9435e-01,  1.4649e-01,  ...,  2.0069e-01,\n",
      "           1.6615e-01, -2.0928e-01],\n",
      "         [ 2.8018e-03, -2.6699e-01,  2.9758e-02,  ...,  4.2184e-02,\n",
      "          -2.1404e-01, -2.7067e-02],\n",
      "         ...,\n",
      "         [ 4.6702e-01, -2.2494e-01, -2.0891e-02,  ..., -1.2467e-01,\n",
      "           1.6480e-01,  1.6840e-02],\n",
      "         [-8.6624e-02, -1.0043e-01,  9.5685e-02,  ...,  2.3607e-01,\n",
      "           7.9408e-02, -1.1790e-01],\n",
      "         [-2.1295e-01, -1.6414e-01, -1.1456e-01,  ..., -3.6400e-02,\n",
      "          -2.1514e-01, -3.8052e-01]]])\n",
      "1 perturbed tensor([[[ 0.2624,  0.1696, -0.0334,  ..., -0.1698, -0.0916, -0.0767],\n",
      "         [ 0.0791,  0.1452,  0.1817,  ...,  0.0429, -0.0390,  0.0810],\n",
      "         [ 0.0939,  0.1365,  0.0625,  ..., -0.1358,  0.0245, -0.1491],\n",
      "         ...,\n",
      "         [ 0.1236, -0.1162, -0.1479,  ...,  0.0775,  0.0450,  0.0506],\n",
      "         [-0.1361,  0.0990, -0.0854,  ..., -0.3044, -0.1874,  0.0627],\n",
      "         [ 0.2432, -0.0970,  0.1927,  ...,  0.1501,  0.1003,  0.2594]],\n",
      "\n",
      "        [[ 0.1033,  0.3339, -0.3240,  ...,  0.1311,  0.3161, -0.2056],\n",
      "         [-0.1229,  0.1847, -0.0302,  ...,  0.2999, -0.0273, -0.2409],\n",
      "         [ 0.1673, -0.0756,  0.0757,  ..., -0.0870, -0.1335, -0.0380],\n",
      "         ...,\n",
      "         [-0.3218, -0.0585, -0.0244,  ...,  0.3451,  0.0537,  0.2498],\n",
      "         [ 0.2326,  0.1361,  0.0766,  ...,  0.0765,  0.1034,  0.0981],\n",
      "         [ 0.0130, -0.2038,  0.1266,  ..., -0.0544,  0.0249, -0.1036]],\n",
      "\n",
      "        [[-0.1988,  0.1789, -0.0410,  ...,  0.0322, -0.1301, -0.0522],\n",
      "         [-0.2375, -0.2145, -0.0414,  ..., -0.3783, -0.0693,  0.0769],\n",
      "         [ 0.0875, -0.0576, -0.0573,  ...,  0.0672, -0.0614,  0.0238],\n",
      "         ...,\n",
      "         [ 0.0743,  0.0150,  0.1354,  ...,  0.1179, -0.1088, -0.0420],\n",
      "         [ 0.1285,  0.1143,  0.0012,  ..., -0.1136,  0.0852, -0.2175],\n",
      "         [-0.1294, -0.1319,  0.0829,  ..., -0.0907,  0.0809, -0.0018]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.1305, -0.0484, -0.2186,  ..., -0.0499,  0.0858, -0.0364],\n",
      "         [-0.1362,  0.1398,  0.1816,  ..., -0.2393,  0.0521, -0.0646],\n",
      "         [-0.1990, -0.0732,  0.1637,  ...,  0.0249,  0.0350,  0.4406],\n",
      "         ...,\n",
      "         [ 0.0254, -0.1961, -0.1964,  ..., -0.0098,  0.1610, -0.1055],\n",
      "         [-0.1599, -0.0482,  0.1282,  ..., -0.0035, -0.1423,  0.2212],\n",
      "         [-0.2139, -0.3253,  0.1555,  ...,  0.1010, -0.2414,  0.0634]],\n",
      "\n",
      "        [[-0.0011, -0.0428,  0.0222,  ..., -0.2006,  0.1508, -0.2418],\n",
      "         [-0.2797,  0.3021, -0.0385,  ..., -0.1367,  0.2008, -0.0413],\n",
      "         [-0.0338,  0.1077,  0.1291,  ...,  0.0218, -0.0940, -0.0983],\n",
      "         ...,\n",
      "         [ 0.0172,  0.0309,  0.2239,  ...,  0.0365,  0.0586,  0.0517],\n",
      "         [-0.2238, -0.1795,  0.3070,  ...,  0.1021, -0.1403, -0.0819],\n",
      "         [-0.0619, -0.0470, -0.0341,  ..., -0.0978, -0.2435, -0.1784]],\n",
      "\n",
      "        [[-0.1056, -0.1176,  0.0234,  ...,  0.1307,  0.0012,  0.0094],\n",
      "         [ 0.3348,  0.3012,  0.1500,  ...,  0.2100,  0.1683, -0.1985],\n",
      "         [ 0.0139, -0.2764,  0.0370,  ...,  0.0386, -0.2173, -0.0297],\n",
      "         ...,\n",
      "         [ 0.4858, -0.2350, -0.0194,  ..., -0.1349,  0.1741,  0.0064],\n",
      "         [-0.0656, -0.1055,  0.1117,  ...,  0.2348,  0.0910, -0.1134],\n",
      "         [-0.2001, -0.1571, -0.1100,  ..., -0.0320, -0.2124, -0.3705]]])\n",
      "2 not perturbed tensor([[[ 0.0009,  0.0482, -0.1008,  ...,  0.0247, -0.0690, -0.2285],\n",
      "         [-0.0353, -0.2294, -0.1258,  ..., -0.0171, -0.1436, -0.0091],\n",
      "         [ 0.0008, -0.0247, -0.0836,  ..., -0.0269,  0.2086,  0.3006],\n",
      "         ...,\n",
      "         [-0.2824,  0.0244, -0.1135,  ...,  0.0203, -0.2257, -0.1085],\n",
      "         [-0.0019, -0.0831, -0.0750,  ...,  0.2922, -0.0120,  0.0153],\n",
      "         [-0.1572,  0.3198,  0.1433,  ..., -0.1506,  0.0822,  0.0486]],\n",
      "\n",
      "        [[ 0.1903, -0.1119, -0.1757,  ..., -0.0059, -0.1956, -0.2449],\n",
      "         [-0.0162,  0.0723, -0.1893,  ...,  0.1034, -0.0168, -0.2517],\n",
      "         [ 0.1613,  0.1441, -0.1287,  ...,  0.1899,  0.1611,  0.0775],\n",
      "         ...,\n",
      "         [ 0.0488, -0.1545,  0.2246,  ..., -0.1690,  0.1620,  0.1217],\n",
      "         [ 0.0608,  0.2456,  0.0402,  ..., -0.0540,  0.0889,  0.0382],\n",
      "         [-0.2830, -0.0245,  0.1647,  ...,  0.2337,  0.0586,  0.1032]],\n",
      "\n",
      "        [[ 0.3663,  0.2109,  0.0395,  ...,  0.0341,  0.0468,  0.1036],\n",
      "         [ 0.3141, -0.1323, -0.0337,  ..., -0.0134, -0.1928,  0.1837],\n",
      "         [ 0.1123,  0.0899, -0.0923,  ...,  0.2067,  0.0444,  0.0599],\n",
      "         ...,\n",
      "         [ 0.3479, -0.2426, -0.4097,  ..., -0.0431, -0.2119, -0.0491],\n",
      "         [-0.1014, -0.1875, -0.0435,  ..., -0.0936, -0.1780, -0.0240],\n",
      "         [ 0.1154, -0.0155,  0.2378,  ..., -0.0631,  0.0411, -0.0062]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0803,  0.2018, -0.2119,  ..., -0.1679,  0.1334,  0.1142],\n",
      "         [-0.1427, -0.2147,  0.0450,  ...,  0.1758,  0.2100,  0.3646],\n",
      "         [ 0.0160,  0.1657, -0.1682,  ..., -0.1628,  0.0044,  0.0702],\n",
      "         ...,\n",
      "         [-0.0680, -0.0188,  0.0219,  ..., -0.1039,  0.2683,  0.0857],\n",
      "         [-0.1302, -0.0354,  0.0676,  ..., -0.1445, -0.3247, -0.0432],\n",
      "         [ 0.1072, -0.1210,  0.2873,  ..., -0.3112,  0.1456, -0.1469]],\n",
      "\n",
      "        [[-0.0937, -0.1128,  0.1716,  ...,  0.0837, -0.2180,  0.0130],\n",
      "         [-0.2501,  0.0555,  0.1359,  ..., -0.1365,  0.2836, -0.0804],\n",
      "         [ 0.0154,  0.0739,  0.0158,  ...,  0.0104,  0.0751,  0.2427],\n",
      "         ...,\n",
      "         [ 0.2419,  0.0651,  0.1636,  ...,  0.2048,  0.0317,  0.0552],\n",
      "         [-0.0248, -0.1420, -0.2248,  ...,  0.1022, -0.1617, -0.2762],\n",
      "         [ 0.0949,  0.0217,  0.0731,  ..., -0.1248, -0.0551,  0.0350]],\n",
      "\n",
      "        [[-0.1325,  0.0931, -0.1947,  ...,  0.1207, -0.2966,  0.0211],\n",
      "         [ 0.2474, -0.0490,  0.0684,  ..., -0.0600,  0.1733,  0.2912],\n",
      "         [-0.0744,  0.0424,  0.0238,  ..., -0.0614,  0.3466,  0.1854],\n",
      "         ...,\n",
      "         [ 0.0606,  0.1958, -0.0034,  ..., -0.0534,  0.0323,  0.0538],\n",
      "         [ 0.1030, -0.0868, -0.1429,  ...,  0.4275, -0.2149,  0.0148],\n",
      "         [ 0.1287,  0.2265,  0.1561,  ..., -0.0721, -0.0645, -0.1679]]])\n",
      "2 perturbed tensor([[[ 2.7855e-03,  3.7618e-02, -9.5447e-02,  ...,  8.5654e-03,\n",
      "          -7.3499e-02, -2.3675e-01],\n",
      "         [-3.4788e-02, -2.4274e-01, -1.0583e-01,  ..., -1.5433e-02,\n",
      "          -1.3723e-01, -6.3636e-03],\n",
      "         [ 9.0836e-03, -2.7787e-02, -9.0077e-02,  ..., -4.0227e-03,\n",
      "           2.1910e-01,  2.9097e-01],\n",
      "         ...,\n",
      "         [-2.9145e-01,  2.8728e-02, -1.2837e-01,  ...,  2.6636e-02,\n",
      "          -2.3249e-01, -1.0994e-01],\n",
      "         [-6.1314e-04, -7.0222e-02, -5.6366e-02,  ...,  2.9294e-01,\n",
      "          -1.3222e-02,  2.2395e-02],\n",
      "         [-1.5688e-01,  3.2510e-01,  1.3659e-01,  ..., -1.4398e-01,\n",
      "           8.6673e-02,  4.1800e-02]],\n",
      "\n",
      "        [[ 1.9476e-01, -1.2492e-01, -1.8883e-01,  ...,  4.5367e-03,\n",
      "          -2.1063e-01, -2.5721e-01],\n",
      "         [-5.8536e-03,  7.8402e-02, -1.9745e-01,  ...,  1.0034e-01,\n",
      "          -2.8305e-02, -2.5744e-01],\n",
      "         [ 1.3633e-01,  1.4359e-01, -1.2348e-01,  ...,  1.9314e-01,\n",
      "           1.3413e-01,  6.7821e-02],\n",
      "         ...,\n",
      "         [ 4.0057e-02, -1.3545e-01,  2.2717e-01,  ..., -1.8708e-01,\n",
      "           1.7257e-01,  1.2357e-01],\n",
      "         [ 4.0035e-02,  2.3849e-01,  3.3665e-02,  ..., -4.0222e-02,\n",
      "           9.1959e-02,  4.8368e-02],\n",
      "         [-2.6081e-01, -3.7350e-02,  1.6791e-01,  ...,  2.5300e-01,\n",
      "           4.9445e-02,  9.5743e-02]],\n",
      "\n",
      "        [[ 3.6478e-01,  2.0998e-01,  4.1883e-02,  ...,  3.6625e-02,\n",
      "           6.2873e-02,  1.1050e-01],\n",
      "         [ 3.1999e-01, -1.2377e-01, -4.9115e-02,  ..., -1.6491e-02,\n",
      "          -2.0384e-01,  1.9153e-01],\n",
      "         [ 1.0719e-01,  7.0133e-02, -9.1220e-02,  ...,  2.0712e-01,\n",
      "           5.7913e-02,  7.7714e-02],\n",
      "         ...,\n",
      "         [ 3.3785e-01, -2.5613e-01, -4.0536e-01,  ..., -6.1523e-02,\n",
      "          -2.1905e-01, -4.2728e-02],\n",
      "         [-1.1376e-01, -1.8661e-01, -4.8310e-02,  ..., -8.9055e-02,\n",
      "          -2.0119e-01, -3.1617e-02],\n",
      "         [ 1.0569e-01, -2.7473e-02,  2.3552e-01,  ..., -4.6527e-02,\n",
      "           4.2638e-02, -1.5474e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 7.5318e-02,  1.9825e-01, -1.9532e-01,  ..., -1.7320e-01,\n",
      "           1.3811e-01,  1.0545e-01],\n",
      "         [-1.4915e-01, -1.9611e-01,  6.0733e-02,  ...,  1.7592e-01,\n",
      "           2.0642e-01,  3.6519e-01],\n",
      "         [ 2.7355e-02,  1.5222e-01, -1.6681e-01,  ..., -1.7831e-01,\n",
      "          -3.2773e-03,  6.5771e-02],\n",
      "         ...,\n",
      "         [-7.3009e-02, -2.9305e-02,  2.1471e-02,  ..., -8.9444e-02,\n",
      "           2.7273e-01,  9.9331e-02],\n",
      "         [-1.3288e-01, -3.8739e-02,  6.6084e-02,  ..., -1.4450e-01,\n",
      "          -3.2699e-01, -5.9031e-02],\n",
      "         [ 1.2419e-01, -1.2133e-01,  2.8421e-01,  ..., -3.2095e-01,\n",
      "           1.3535e-01, -1.3255e-01]],\n",
      "\n",
      "        [[-9.3718e-02, -1.2594e-01,  1.7542e-01,  ...,  8.4776e-02,\n",
      "          -2.2226e-01,  1.6017e-02],\n",
      "         [-2.3701e-01,  4.4098e-02,  1.2454e-01,  ..., -1.4612e-01,\n",
      "           2.9672e-01, -6.5683e-02],\n",
      "         [ 1.1948e-02,  7.0104e-02,  3.5381e-02,  ..., -3.2019e-06,\n",
      "           5.9693e-02,  2.3153e-01],\n",
      "         ...,\n",
      "         [ 2.2560e-01,  4.7756e-02,  1.4965e-01,  ...,  2.1394e-01,\n",
      "           1.7273e-02,  6.0956e-02],\n",
      "         [-1.5436e-02, -1.3454e-01, -2.2466e-01,  ...,  9.3292e-02,\n",
      "          -1.6018e-01, -3.1197e-01],\n",
      "         [ 8.2639e-02,  2.4339e-02,  7.8043e-02,  ..., -1.0793e-01,\n",
      "          -7.2524e-02,  4.1696e-02]],\n",
      "\n",
      "        [[-1.5190e-01,  9.1078e-02, -2.2251e-01,  ...,  1.3191e-01,\n",
      "          -3.0059e-01,  2.4816e-02],\n",
      "         [ 2.5050e-01, -4.3863e-02,  5.7803e-02,  ..., -3.7773e-02,\n",
      "           1.7569e-01,  2.9862e-01],\n",
      "         [-8.0168e-02,  5.0623e-02,  1.3491e-02,  ..., -5.6074e-02,\n",
      "           3.4275e-01,  1.8892e-01],\n",
      "         ...,\n",
      "         [ 6.5649e-02,  2.1011e-01,  7.1238e-03,  ..., -5.3081e-02,\n",
      "           4.5762e-02,  4.5066e-02],\n",
      "         [ 9.1391e-02, -8.3405e-02, -1.2616e-01,  ...,  4.4337e-01,\n",
      "          -2.0189e-01,  2.6080e-02],\n",
      "         [ 1.2422e-01,  2.1758e-01,  1.5517e-01,  ..., -9.3029e-02,\n",
      "          -8.0766e-02, -1.6270e-01]]])\n",
      "3 not perturbed tensor([[[-0.0973],\n",
      "         [ 0.2615],\n",
      "         [-0.0952],\n",
      "         [ 0.4682]],\n",
      "\n",
      "        [[-0.1409],\n",
      "         [ 0.0945],\n",
      "         [-0.0829],\n",
      "         [-0.1311]],\n",
      "\n",
      "        [[-0.2385],\n",
      "         [-0.0389],\n",
      "         [ 0.1176],\n",
      "         [ 0.0792]],\n",
      "\n",
      "        [[ 0.1175],\n",
      "         [ 0.2824],\n",
      "         [ 0.0736],\n",
      "         [-0.1335]]])\n",
      "3 perturbed tensor([[[-0.1067],\n",
      "         [ 0.2703],\n",
      "         [-0.0991],\n",
      "         [ 0.4796]],\n",
      "\n",
      "        [[-0.1552],\n",
      "         [ 0.0831],\n",
      "         [-0.0848],\n",
      "         [-0.1308]],\n",
      "\n",
      "        [[-0.2399],\n",
      "         [-0.0487],\n",
      "         [ 0.1179],\n",
      "         [ 0.0822]],\n",
      "\n",
      "        [[ 0.1292],\n",
      "         [ 0.2929],\n",
      "         [ 0.0780],\n",
      "         [-0.1348]]])\n",
      "4\n",
      "0 not perturbed tensor([[[-0.0371,  0.2360, -0.2249,  0.2208,  0.0131, -0.0041, -0.0559,\n",
      "          -0.2566,  0.1075, -0.0457, -0.0374, -0.0574, -0.0383,  0.1642,\n",
      "          -0.1150, -0.0147,  0.0685,  0.2135,  0.1374, -0.2049]],\n",
      "\n",
      "        [[-0.1726,  0.1316,  0.3769,  0.0227, -0.0971,  0.1329,  0.1101,\n",
      "          -0.3409,  0.1156,  0.0013,  0.0370, -0.1055, -0.0273,  0.1994,\n",
      "           0.1718, -0.0474,  0.1423,  0.0406, -0.3865, -0.1975]],\n",
      "\n",
      "        [[-0.3282,  0.0450, -0.0844,  0.2203, -0.2181, -0.2125, -0.1503,\n",
      "           0.1951, -0.1167, -0.0397,  0.0676,  0.0531,  0.2069,  0.2211,\n",
      "           0.1939, -0.2043,  0.1752,  0.0997, -0.0796,  0.1063]],\n",
      "\n",
      "        [[ 0.0057, -0.0790, -0.1181, -0.2842,  0.0800,  0.0078, -0.2639,\n",
      "           0.1107, -0.1194, -0.0158,  0.0257, -0.0112,  0.1499,  0.0582,\n",
      "          -0.1410, -0.1108, -0.2060, -0.1912,  0.1195, -0.0223]]])\n",
      "0 perturbed tensor([[[-0.0115,  0.2350, -0.1992,  0.2198,  0.0015, -0.0007, -0.0532,\n",
      "          -0.2576,  0.1209, -0.0418, -0.0366, -0.0530, -0.0189,  0.1711,\n",
      "          -0.1357, -0.0125,  0.0714,  0.2132,  0.1408, -0.2005]],\n",
      "\n",
      "        [[-0.1566,  0.1145,  0.3757,  0.0072, -0.1023,  0.1361,  0.1065,\n",
      "          -0.3335,  0.1004,  0.0071,  0.0396, -0.0988, -0.0083,  0.1717,\n",
      "           0.1762, -0.0376,  0.1348,  0.0344, -0.3764, -0.1877]],\n",
      "\n",
      "        [[-0.3512,  0.0284, -0.0964,  0.2134, -0.2164, -0.2160, -0.1473,\n",
      "           0.1979, -0.1266, -0.0316,  0.0608,  0.0541,  0.2086,  0.2365,\n",
      "           0.1882, -0.1898,  0.1667,  0.0962, -0.0785,  0.1141]],\n",
      "\n",
      "        [[ 0.0046, -0.0761, -0.1290, -0.2712,  0.0668,  0.0161, -0.2571,\n",
      "           0.1014, -0.1165,  0.0021,  0.0166, -0.0084,  0.1514,  0.0526,\n",
      "          -0.1290, -0.1194, -0.2063, -0.1927,  0.1184, -0.0229]]])\n",
      "1 not perturbed tensor([[[ 1.1437e-02,  1.2526e-01,  7.2381e-02,  1.3750e-01,  1.1557e-01,\n",
      "           1.2034e-01, -8.5850e-02, -9.3608e-02,  2.2503e-01, -1.1305e-02,\n",
      "          -1.9952e-01, -2.0645e-01,  2.3427e-01,  4.2596e-02,  3.8518e-01,\n",
      "          -1.2609e-01, -6.9444e-03, -2.0242e-01, -7.1196e-02, -1.2764e-01],\n",
      "         [-1.1417e-01, -2.9275e-01, -4.3490e-02,  1.5845e-01,  7.4777e-02,\n",
      "          -7.3601e-02,  4.5169e-02, -3.0192e-02,  8.3239e-03, -1.2076e-01,\n",
      "           2.1221e-01,  2.0990e-01, -3.7528e-02,  3.7959e-01,  4.1998e-02,\n",
      "           1.7752e-01,  4.9847e-02, -1.1505e-01,  3.2803e-02, -4.5449e-02],\n",
      "         [-6.2694e-02,  1.2959e-01,  1.0879e-01, -7.5175e-03,  8.8354e-02,\n",
      "           1.0690e-02,  2.8755e-01,  8.8700e-03, -5.1217e-02,  3.6820e-02,\n",
      "           1.3198e-01,  1.8777e-01, -6.0452e-02,  1.4551e-01,  2.8593e-01,\n",
      "          -2.9300e-01, -2.9334e-01, -2.3751e-01, -1.1827e-01,  1.0836e-01],\n",
      "         [ 2.5390e-02, -1.5884e-01,  1.0073e-01,  1.4967e-01, -2.9183e-01,\n",
      "           1.7161e-01,  4.4546e-03, -4.6732e-02,  1.5440e-01,  1.9488e-02,\n",
      "           2.2579e-03,  4.0566e-02, -2.1134e-01, -6.7200e-02,  1.7870e-01,\n",
      "          -1.6359e-01, -4.3367e-02, -8.9980e-02, -9.3021e-02,  3.1673e-01],\n",
      "         [-1.8343e-01, -1.7499e-01,  7.4419e-03,  2.4698e-02, -1.0056e-02,\n",
      "          -3.3787e-02, -2.5928e-01,  3.4603e-02,  8.4084e-03, -5.8316e-02,\n",
      "          -1.7883e-01, -4.0997e-01,  1.6508e-01,  3.8838e-02,  5.9883e-02,\n",
      "          -7.2030e-02, -3.0822e-02, -2.0999e-01, -3.3749e-01,  1.4667e-01]],\n",
      "\n",
      "        [[ 7.4800e-02,  1.2491e-01,  3.9559e-01,  4.7158e-02, -1.6368e-01,\n",
      "          -1.0787e-01, -3.2126e-01, -5.3242e-03,  1.0625e-01,  4.9022e-02,\n",
      "           1.2754e-01, -2.2954e-01, -1.1062e-02, -6.6402e-02,  1.2742e-01,\n",
      "           1.3732e-01,  1.9013e-01,  2.4288e-01, -2.1081e-01,  1.8732e-01],\n",
      "         [ 2.4537e-02, -9.4162e-02,  9.4164e-02, -4.5084e-02,  8.6540e-02,\n",
      "          -1.9969e-01,  2.2083e-01,  2.5095e-01, -3.3884e-01,  3.8639e-02,\n",
      "           6.8267e-02, -5.0188e-02,  1.5568e-01,  1.9332e-02,  1.7554e-02,\n",
      "           9.5341e-02,  5.4956e-02, -1.6493e-01,  8.5700e-02,  1.2668e-01],\n",
      "         [ 5.8410e-02,  1.0939e-01, -1.9051e-01, -2.1623e-01,  6.0228e-02,\n",
      "           8.1857e-02, -7.5676e-02, -9.6439e-02, -1.2986e-01,  1.1640e-01,\n",
      "           1.2377e-02, -5.2919e-02, -1.6013e-01, -7.3253e-02,  3.1310e-02,\n",
      "          -1.6323e-01, -5.8588e-02,  2.9712e-01,  1.1103e-01,  1.8125e-01],\n",
      "         [ 9.7590e-02,  2.5347e-02,  7.0086e-02,  1.6364e-01,  1.6960e-01,\n",
      "          -3.3933e-01,  7.4035e-02,  5.2197e-02,  9.3092e-02, -9.4201e-03,\n",
      "           3.5007e-01, -7.7151e-02, -1.5084e-01,  7.6367e-03,  1.5136e-02,\n",
      "          -5.5627e-02,  2.0151e-01, -3.7537e-03, -1.8203e-03, -5.7476e-02],\n",
      "         [-1.3202e-01, -7.7442e-02,  3.0237e-01, -1.4220e-01, -8.7429e-02,\n",
      "           2.8191e-02, -3.6006e-01,  8.4909e-02,  2.1446e-02,  1.1769e-02,\n",
      "          -5.4913e-02,  1.3876e-01, -9.4132e-02,  7.4400e-02, -1.2398e-01,\n",
      "           9.7931e-02,  1.1004e-01,  1.0284e-01, -1.4165e-01,  6.0909e-02]],\n",
      "\n",
      "        [[-6.0064e-02, -1.7845e-01,  1.5524e-01,  2.2086e-01, -8.9849e-02,\n",
      "          -8.2459e-02, -2.4928e-01, -1.2540e-01,  6.4268e-02, -1.1351e-02,\n",
      "          -3.4174e-01, -3.4301e-01,  5.1549e-02, -5.7379e-02,  9.4622e-02,\n",
      "           1.3300e-01,  7.4040e-02,  4.4137e-02, -1.0936e-01,  2.5333e-01],\n",
      "         [-3.0760e-02, -1.8556e-02, -2.2858e-01,  5.2222e-02, -2.2352e-01,\n",
      "           1.9879e-01,  6.0176e-02, -6.9803e-02, -2.3899e-01,  4.9344e-02,\n",
      "           1.6113e-01, -6.7064e-03, -9.9122e-02, -2.4513e-01, -1.9654e-01,\n",
      "           1.1010e-01,  2.1262e-01,  2.7884e-01, -1.7634e-01,  1.0072e-01],\n",
      "         [-5.2570e-03,  1.2458e-01,  2.2115e-02, -1.3038e-01, -3.8985e-01,\n",
      "           9.5055e-02,  1.2596e-01, -1.0276e-02, -8.8053e-02, -1.1825e-01,\n",
      "          -7.5146e-03,  1.4941e-01, -1.4373e-01, -2.8979e-02,  3.5006e-01,\n",
      "          -3.9303e-02, -1.2430e-01, -1.7972e-01,  1.4980e-01,  3.2464e-02],\n",
      "         [-8.9318e-02, -1.5117e-01,  2.0121e-01,  3.6326e-02, -2.2860e-01,\n",
      "          -7.3898e-02, -2.8777e-02,  9.6631e-02, -3.4403e-02, -1.7943e-01,\n",
      "           1.3021e-02,  1.0155e-02, -5.0899e-03, -3.5287e-02, -2.3654e-01,\n",
      "           1.5650e-01,  3.8433e-02,  1.1818e-01,  1.0184e-01, -2.4620e-01],\n",
      "         [ 6.7554e-02, -1.1029e-01,  8.0749e-02, -1.4012e-01,  2.2655e-01,\n",
      "           5.0800e-02, -1.1539e-01, -4.7064e-02, -2.4603e-01, -2.1741e-01,\n",
      "           1.9576e-01,  2.3327e-02, -6.2670e-02, -1.2876e-01,  2.1950e-01,\n",
      "           8.3267e-02, -2.3110e-03, -7.4189e-02, -1.4998e-01, -1.0101e-01]],\n",
      "\n",
      "        [[-3.4072e-01,  1.6709e-01, -1.2455e-01, -2.3231e-02,  3.6214e-01,\n",
      "           1.0056e-02, -9.3732e-02,  2.3665e-01, -9.5089e-03,  2.9072e-02,\n",
      "          -2.5478e-01,  1.1043e-01, -1.4707e-01, -9.0562e-02, -1.2168e-01,\n",
      "           1.5705e-02, -1.3598e-01,  4.3109e-02, -1.2779e-01, -1.1672e-01],\n",
      "         [-6.3635e-02, -1.4823e-01,  2.3210e-01,  8.4515e-02,  1.6656e-01,\n",
      "           2.3142e-01, -1.0833e-01,  1.8744e-01, -1.2134e-01, -3.1634e-02,\n",
      "          -1.0964e-01,  7.3567e-02, -2.0032e-02,  6.4478e-02, -3.2666e-01,\n",
      "          -2.3045e-01,  1.8638e-01, -2.0615e-01,  8.5948e-02, -2.2746e-03],\n",
      "         [-6.5586e-02,  7.9710e-02,  1.4419e-01,  3.9309e-02,  7.4573e-02,\n",
      "           2.3384e-02, -6.8806e-02,  5.2876e-02,  1.3725e-01, -1.8910e-01,\n",
      "          -8.9208e-02,  1.1800e-01,  6.4110e-03, -2.9936e-01, -1.0487e-01,\n",
      "           1.5632e-01,  2.0738e-01,  2.9939e-02, -2.0537e-01, -1.1076e-01],\n",
      "         [ 1.0030e-02,  2.5413e-02, -1.6328e-01,  1.0284e-02,  9.2461e-02,\n",
      "          -8.5208e-03,  1.6182e-01, -1.5492e-01, -2.3166e-02, -1.0244e-01,\n",
      "           4.1574e-01, -1.0340e-01, -1.3286e-01,  1.9119e-01,  8.8091e-02,\n",
      "           1.6912e-01, -1.0861e-01,  1.8972e-02,  2.8028e-01, -1.4525e-01],\n",
      "         [-1.5261e-01, -2.8641e-02,  1.2178e-01,  1.8118e-01, -2.0266e-01,\n",
      "          -9.3158e-02, -3.2414e-02,  1.7004e-01,  2.0790e-01,  1.0681e-01,\n",
      "           1.4319e-01,  1.2915e-01,  1.4681e-02, -5.2412e-02,  1.9288e-01,\n",
      "           3.4449e-02, -1.6110e-01,  1.9943e-01,  1.6117e-01, -1.8898e-03]],\n",
      "\n",
      "        [[ 4.7937e-02, -4.2589e-02,  2.5004e-02,  1.3717e-01,  2.9061e-01,\n",
      "           4.1555e-02, -1.4485e-01, -1.0911e-01, -6.6800e-02, -8.6985e-02,\n",
      "           8.4555e-02,  2.6841e-01, -2.7070e-01,  1.8772e-01, -3.9793e-02,\n",
      "          -2.0392e-01,  4.7623e-02,  1.0328e-01, -1.6821e-01, -1.2582e-01],\n",
      "         [ 2.6659e-01, -4.9667e-02,  3.4200e-01,  9.4852e-03,  1.2868e-01,\n",
      "           1.9176e-01,  9.7064e-03, -1.9171e-01, -5.9350e-02, -1.0156e-02,\n",
      "          -4.9611e-02,  7.4007e-02,  1.3193e-01,  1.3703e-01, -2.1925e-01,\n",
      "           9.8889e-02, -5.6551e-02,  2.0505e-01, -5.5431e-02, -3.0712e-01],\n",
      "         [ 6.2491e-02,  2.4166e-01,  1.4361e-01,  5.4705e-02,  9.7074e-02,\n",
      "          -8.1415e-02, -8.3027e-02,  2.5370e-01,  6.7260e-02,  6.2046e-02,\n",
      "           2.1082e-01, -7.9075e-02, -6.7552e-02, -6.2731e-02,  9.3424e-02,\n",
      "          -1.9367e-01, -9.7471e-02, -1.7693e-02,  5.3288e-02,  5.5840e-03],\n",
      "         [ 1.0116e-01, -2.2655e-01, -1.2462e-01,  1.0495e-01,  1.0218e-01,\n",
      "          -1.5186e-01, -9.6500e-02, -2.8528e-02,  4.9645e-02,  3.1180e-01,\n",
      "          -1.4714e-01,  9.0580e-02,  4.4076e-02, -1.1299e-02, -9.0766e-02,\n",
      "           1.4419e-01,  1.0761e-01,  1.4596e-01,  6.2348e-02,  3.6850e-02],\n",
      "         [ 6.8856e-02,  1.0136e-01, -8.1042e-02, -2.0343e-01,  5.8815e-02,\n",
      "          -4.6933e-02, -6.5544e-02,  3.3870e-01, -1.4472e-01,  1.2143e-01,\n",
      "           1.0945e-01,  2.7237e-01,  2.0104e-01, -1.8397e-01,  1.3260e-01,\n",
      "          -1.3358e-01, -1.2044e-01,  7.8391e-02,  1.4454e-01, -1.8491e-01]],\n",
      "\n",
      "        [[-7.1219e-02,  1.7106e-02, -9.6819e-02,  3.4641e-02, -3.2807e-02,\n",
      "          -3.1498e-01,  5.5692e-02, -1.9390e-02, -4.0187e-02, -2.6433e-01,\n",
      "           1.0972e-01,  9.3100e-03,  2.4860e-01, -2.7434e-01, -3.5594e-02,\n",
      "           1.2670e-02,  6.1087e-02,  5.7134e-02, -2.8703e-01, -1.9219e-01],\n",
      "         [-2.3083e-01,  4.4239e-02,  4.0585e-02, -1.5125e-01, -8.9132e-02,\n",
      "          -1.1792e-01, -3.4257e-01, -1.9058e-01, -1.1617e-01,  9.2995e-02,\n",
      "          -4.0124e-02,  1.2901e-01,  3.0697e-01,  9.7371e-02, -2.1627e-01,\n",
      "           5.4122e-02, -3.0405e-02, -3.9649e-02, -3.0860e-01,  6.9593e-02],\n",
      "         [-1.3473e-01, -7.1159e-02, -2.3183e-01, -2.4482e-02, -1.2664e-01,\n",
      "          -1.3400e-01, -8.1177e-02,  6.1499e-02,  3.0608e-01,  2.1502e-01,\n",
      "          -5.7258e-02, -2.9913e-01, -2.8329e-01, -1.8079e-01, -2.0962e-02,\n",
      "           7.8301e-02,  2.9893e-02, -3.6208e-03,  1.8304e-01,  3.0835e-02],\n",
      "         [ 1.7721e-01, -2.9785e-02, -1.0926e-02,  1.7837e-01, -8.8294e-03,\n",
      "          -2.1154e-02,  1.1077e-01,  1.0266e-01,  1.1143e-01,  2.3234e-02,\n",
      "          -2.5009e-01, -1.1298e-01, -2.1351e-03, -3.1110e-01,  1.5129e-01,\n",
      "          -2.8083e-02, -3.2575e-02,  3.5145e-02, -9.7865e-02, -2.9233e-02],\n",
      "         [ 1.8106e-01, -3.6034e-01,  9.6411e-03, -1.2126e-01,  1.4092e-01,\n",
      "           1.2660e-01,  2.7227e-01, -2.3154e-02, -1.1944e-01, -1.1519e-01,\n",
      "          -1.3557e-01, -9.7866e-02,  5.4384e-02, -7.6864e-02,  6.1034e-02,\n",
      "          -2.2516e-01, -8.7901e-02,  2.2586e-01,  5.8987e-02, -2.2012e-01]],\n",
      "\n",
      "        [[-4.5412e-01,  8.9327e-02, -7.9779e-02, -4.2556e-01,  1.6234e-01,\n",
      "          -2.5672e-01, -1.0066e-01,  1.6070e-02, -9.0654e-02,  1.7698e-01,\n",
      "           4.1090e-02, -2.4258e-01, -3.2412e-01, -1.4499e-02,  5.2326e-02,\n",
      "          -8.4332e-02, -1.1930e-01,  2.2331e-01, -1.3889e-01,  1.3566e-01],\n",
      "         [-4.0366e-01,  7.1417e-02, -1.5186e-03,  2.5037e-01,  4.9946e-02,\n",
      "           1.7284e-01,  3.7053e-02, -1.5102e-01,  2.0544e-01,  6.3623e-02,\n",
      "          -1.9806e-01, -2.8456e-01, -3.9189e-02,  3.3718e-02,  1.1189e-01,\n",
      "          -7.7332e-02, -9.2966e-02, -1.8199e-01, -8.2332e-02,  1.6991e-01],\n",
      "         [-2.2577e-01, -1.1130e-01,  1.1883e-01,  7.4041e-02,  1.0607e-01,\n",
      "           1.3338e-01,  6.1235e-03,  2.3008e-01, -6.8862e-02, -9.1675e-02,\n",
      "          -4.2628e-02,  1.8588e-02, -3.1521e-01,  5.6936e-02,  6.8340e-02,\n",
      "          -9.8108e-02,  2.9766e-01, -2.0591e-01,  5.7688e-02, -1.4055e-01],\n",
      "         [-3.3925e-03,  7.2122e-02,  3.0146e-01, -7.7006e-02, -7.5199e-02,\n",
      "           3.6794e-02, -1.9932e-03, -4.7769e-02, -1.4135e-02, -2.1766e-01,\n",
      "          -9.7920e-02, -8.0509e-02,  1.2744e-02, -1.6275e-03, -5.5776e-02,\n",
      "          -2.4219e-01, -1.5307e-01, -1.3903e-01,  4.6296e-02, -1.4368e-01],\n",
      "         [-1.4586e-01, -1.4272e-01,  7.5357e-02,  2.2050e-01, -2.1643e-02,\n",
      "          -1.1800e-01, -1.3970e-01,  7.4183e-02, -3.8817e-02, -3.2870e-01,\n",
      "          -5.1935e-02,  8.4885e-03,  1.2176e-01,  1.9595e-01, -4.2353e-02,\n",
      "          -3.8357e-02,  5.4538e-02,  2.6975e-01,  1.2301e-01,  8.9047e-02]],\n",
      "\n",
      "        [[ 2.9032e-02, -2.7356e-02, -1.0450e-01,  3.2640e-02,  2.9241e-01,\n",
      "          -6.3829e-02, -1.6679e-01,  7.7876e-02, -1.2387e-01, -8.6020e-02,\n",
      "           2.4306e-01,  5.4288e-02, -1.4956e-01,  1.2739e-02, -2.3038e-01,\n",
      "          -3.5848e-02, -1.2324e-01, -1.8217e-01,  2.7795e-02,  2.1963e-01],\n",
      "         [-2.3894e-01,  3.7642e-02,  1.9515e-02,  5.9815e-02, -2.8645e-02,\n",
      "           1.2850e-01,  1.9899e-01, -7.8228e-02,  4.2016e-01, -2.2860e-01,\n",
      "           9.1023e-02, -2.6532e-01, -2.0217e-02, -2.2041e-01,  3.9038e-02,\n",
      "          -9.5832e-02, -1.0357e-01,  2.3136e-01, -2.6509e-01, -9.5663e-02],\n",
      "         [ 1.8936e-02, -5.5291e-02, -1.9853e-01,  2.4827e-01, -1.7003e-01,\n",
      "          -1.2657e-01,  1.0823e-01, -1.8247e-01,  9.4780e-02,  4.7715e-02,\n",
      "          -2.3195e-01,  2.0140e-01, -1.7159e-01, -2.2349e-01, -3.4049e-02,\n",
      "          -1.0581e-02,  6.2383e-02, -8.9361e-02, -2.3677e-02, -1.4923e-02],\n",
      "         [-2.7633e-02,  1.0750e-01,  7.2752e-02, -2.5408e-02, -1.4783e-01,\n",
      "           5.6800e-02, -3.3345e-02, -1.9717e-02,  2.2956e-01,  5.9270e-02,\n",
      "           2.2438e-01, -4.6638e-02, -1.7895e-01, -1.0411e-03, -1.3480e-01,\n",
      "           1.8112e-01,  8.3270e-03,  2.2350e-02, -1.1356e-02,  1.0691e-01],\n",
      "         [-4.3126e-01, -1.2378e-01, -1.7825e-01, -2.0000e-01,  9.6840e-02,\n",
      "          -2.1166e-03, -2.7294e-01,  7.0143e-03, -9.6045e-02, -1.1073e-01,\n",
      "           2.8866e-01, -1.8220e-01,  1.0385e-01,  4.1659e-04, -1.6353e-01,\n",
      "           4.9845e-02, -2.4715e-01, -1.2384e-01, -1.5657e-01,  1.0021e-01]]])\n",
      "1 perturbed tensor([[[ 9.2304e-03,  1.1754e-01,  8.6705e-02,  1.5771e-01,  1.2127e-01,\n",
      "           1.2056e-01, -8.9471e-02, -1.0125e-01,  2.4011e-01, -4.8336e-03,\n",
      "          -2.0888e-01, -2.1284e-01,  2.2639e-01,  2.4733e-02,  3.8247e-01,\n",
      "          -1.1725e-01,  1.1794e-03, -1.9870e-01, -8.1470e-02, -1.2854e-01],\n",
      "         [-1.1567e-01, -2.9923e-01, -3.7046e-02,  1.4938e-01,  6.9873e-02,\n",
      "          -7.7375e-02,  3.6008e-02, -2.0640e-02,  1.6300e-03, -1.3059e-01,\n",
      "           2.0737e-01,  2.0404e-01, -2.4327e-02,  3.7450e-01,  2.6529e-02,\n",
      "           1.9545e-01,  4.2455e-02, -1.2830e-01,  2.7661e-02, -4.6008e-02],\n",
      "         [-4.8078e-02,  1.3091e-01,  1.1020e-01, -2.5549e-03,  9.6497e-02,\n",
      "           2.8739e-03,  3.0201e-01,  2.0352e-03, -4.9933e-02,  4.4916e-02,\n",
      "           1.4761e-01,  1.7914e-01, -4.4954e-02,  1.3721e-01,  2.9113e-01,\n",
      "          -2.8794e-01, -2.9407e-01, -2.4207e-01, -1.4250e-01,  1.2094e-01],\n",
      "         [ 3.6678e-02, -1.6633e-01,  7.8454e-02,  1.5526e-01, -2.8642e-01,\n",
      "           1.6769e-01,  1.4009e-02, -3.0960e-02,  1.6515e-01,  2.0348e-02,\n",
      "           7.5602e-03,  5.3402e-02, -2.0365e-01, -6.8221e-02,  1.8763e-01,\n",
      "          -1.6872e-01, -2.2608e-02, -1.0011e-01, -9.6635e-02,  3.2000e-01],\n",
      "         [-1.8210e-01, -1.7117e-01,  5.3100e-03,  3.3653e-02, -3.9715e-03,\n",
      "          -4.1047e-02, -2.5867e-01,  4.2891e-02,  3.1453e-02, -5.6411e-02,\n",
      "          -1.7134e-01, -4.0661e-01,  1.6339e-01,  3.8938e-02,  6.1848e-02,\n",
      "          -5.9771e-02, -2.9255e-02, -2.2385e-01, -3.3190e-01,  1.3973e-01]],\n",
      "\n",
      "        [[ 6.7030e-02,  1.2942e-01,  3.7211e-01,  4.8704e-02, -1.4182e-01,\n",
      "          -1.1028e-01, -3.2464e-01,  6.4310e-03,  1.1512e-01,  4.6994e-02,\n",
      "           1.3491e-01, -2.3564e-01, -3.7250e-03, -6.3225e-02,  1.3845e-01,\n",
      "           1.5071e-01,  1.8943e-01,  2.2618e-01, -2.1413e-01,  1.9276e-01],\n",
      "         [ 1.8317e-02, -9.0784e-02,  8.6491e-02, -4.9016e-02,  9.3117e-02,\n",
      "          -1.9627e-01,  2.2174e-01,  2.4368e-01, -3.4234e-01,  2.6795e-02,\n",
      "           6.6266e-02, -5.3750e-02,  1.5799e-01,  4.0225e-02, -1.5148e-03,\n",
      "           9.6189e-02,  5.4115e-02, -1.7090e-01,  9.7807e-02,  1.4914e-01],\n",
      "         [ 8.0754e-02,  1.0702e-01, -1.8082e-01, -2.1891e-01,  6.6097e-02,\n",
      "           8.4919e-02, -7.0516e-02, -9.0518e-02, -1.1777e-01,  1.2964e-01,\n",
      "           8.9697e-03, -4.2725e-02, -1.5368e-01, -6.8751e-02,  2.6737e-02,\n",
      "          -1.6520e-01, -7.4680e-02,  3.0494e-01,  1.0803e-01,  1.7078e-01],\n",
      "         [ 9.8526e-02,  2.7856e-02,  6.9100e-02,  1.6397e-01,  1.7014e-01,\n",
      "          -3.3750e-01,  8.8145e-02,  5.1924e-02,  1.0075e-01, -1.3895e-02,\n",
      "           3.4186e-01, -6.6103e-02, -1.5924e-01, -1.1722e-02,  1.0406e-02,\n",
      "          -4.9037e-02,  2.0699e-01,  6.0926e-03, -2.0756e-02, -5.2732e-02],\n",
      "         [-1.2577e-01, -8.6089e-02,  3.1108e-01, -1.2906e-01, -8.0898e-02,\n",
      "           4.5938e-02, -3.5497e-01,  8.2399e-02,  3.0586e-02, -6.5717e-03,\n",
      "          -4.9801e-02,  1.1433e-01, -8.3137e-02,  6.5728e-02, -1.3256e-01,\n",
      "           8.9396e-02,  1.1157e-01,  9.1433e-02, -1.3990e-01,  5.1475e-02]],\n",
      "\n",
      "        [[-6.9311e-02, -2.0691e-01,  1.4666e-01,  2.1347e-01, -8.8043e-02,\n",
      "          -9.2711e-02, -2.5353e-01, -1.1546e-01,  6.1508e-02, -1.9817e-03,\n",
      "          -3.5045e-01, -3.4432e-01,  3.7574e-02, -6.3937e-02,  8.3038e-02,\n",
      "           1.4044e-01,  6.8968e-02,  4.8399e-02, -1.1367e-01,  2.4019e-01],\n",
      "         [-2.6961e-02, -1.1147e-02, -2.4630e-01,  3.0668e-02, -2.2414e-01,\n",
      "           1.7558e-01,  6.4209e-02, -8.1490e-02, -2.4648e-01,  5.6894e-02,\n",
      "           1.5045e-01, -6.2752e-03, -8.9825e-02, -2.5082e-01, -1.8907e-01,\n",
      "           1.1591e-01,  1.9794e-01,  2.9738e-01, -1.7626e-01,  9.5385e-02],\n",
      "         [-2.6182e-02,  1.3651e-01,  3.0152e-02, -1.2067e-01, -3.9209e-01,\n",
      "           1.0209e-01,  1.2713e-01, -1.5443e-02, -7.4040e-02, -1.1141e-01,\n",
      "          -3.0595e-02,  1.4018e-01, -1.3873e-01, -7.7911e-03,  3.5509e-01,\n",
      "          -5.8378e-02, -1.2374e-01, -1.6347e-01,  1.3700e-01,  3.8096e-02],\n",
      "         [-9.7134e-02, -1.6821e-01,  2.1299e-01,  2.2273e-02, -2.3838e-01,\n",
      "          -7.1109e-02, -3.0513e-02,  9.2864e-02, -2.9636e-02, -1.8401e-01,\n",
      "           1.3241e-02, -3.4487e-03,  9.6165e-03, -4.6795e-02, -2.5508e-01,\n",
      "           1.6870e-01,  2.8551e-02,  1.1440e-01,  1.1400e-01, -2.4766e-01],\n",
      "         [ 8.0159e-02, -1.1577e-01,  8.3564e-02, -1.2741e-01,  2.1911e-01,\n",
      "           6.8492e-02, -1.0296e-01, -4.8464e-02, -2.4659e-01, -2.1202e-01,\n",
      "           1.8693e-01,  3.5557e-02, -4.3309e-02, -1.2958e-01,  2.2547e-01,\n",
      "           8.0449e-02, -1.3986e-03, -7.4165e-02, -1.4380e-01, -1.0564e-01]],\n",
      "\n",
      "        [[-3.5505e-01,  1.5397e-01, -1.3366e-01, -1.6594e-02,  3.6371e-01,\n",
      "           2.0018e-02, -7.7710e-02,  2.5326e-01, -3.7374e-03,  1.5558e-02,\n",
      "          -2.5916e-01,  1.0753e-01, -1.5412e-01, -1.0743e-01, -1.1147e-01,\n",
      "           2.9250e-02, -1.3611e-01,  4.0021e-02, -1.3172e-01, -1.1649e-01],\n",
      "         [-3.6341e-02, -1.4886e-01,  2.2767e-01,  9.6499e-02,  1.7192e-01,\n",
      "           2.3669e-01, -1.0240e-01,  1.8929e-01, -1.2297e-01, -3.0933e-02,\n",
      "          -1.0349e-01,  8.4374e-02, -1.3617e-02,  5.8473e-02, -3.0843e-01,\n",
      "          -2.2753e-01,  1.9998e-01, -2.0766e-01,  9.3681e-02,  1.0106e-03],\n",
      "         [-6.2986e-02,  7.4673e-02,  1.4256e-01,  2.5197e-02,  7.8066e-02,\n",
      "           5.0577e-03, -8.7980e-02,  7.8405e-02,  1.3385e-01, -1.8178e-01,\n",
      "          -9.3831e-02,  1.1855e-01, -1.0888e-02, -3.0372e-01, -8.7590e-02,\n",
      "           1.5493e-01,  1.9114e-01,  3.8979e-02, -2.1200e-01, -1.1737e-01],\n",
      "         [ 1.7733e-02,  8.9584e-03, -1.5855e-01,  1.7634e-02,  9.6051e-02,\n",
      "           1.0728e-02,  1.6649e-01, -1.6456e-01, -4.1967e-02, -9.2797e-02,\n",
      "           4.3061e-01, -9.7454e-02, -1.3994e-01,  2.0064e-01,  7.9321e-02,\n",
      "           1.6451e-01, -1.0090e-01,  1.0804e-02,  2.6928e-01, -1.5583e-01],\n",
      "         [-1.5433e-01, -3.9255e-02,  1.2059e-01,  1.9793e-01, -2.0070e-01,\n",
      "          -8.8946e-02, -2.8911e-02,  1.4845e-01,  2.1064e-01,  9.6831e-02,\n",
      "           1.3433e-01,  1.3880e-01,  2.9584e-02, -4.4801e-02,  1.8748e-01,\n",
      "           2.0943e-02, -1.4721e-01,  2.0171e-01,  1.6659e-01, -1.1984e-02]],\n",
      "\n",
      "        [[ 3.9100e-02, -5.4985e-02,  3.9528e-02,  1.4163e-01,  2.6696e-01,\n",
      "           3.7783e-02, -1.4294e-01, -1.1845e-01, -7.7258e-02, -7.8668e-02,\n",
      "           8.4422e-02,  2.7423e-01, -2.5694e-01,  1.8683e-01, -4.1855e-02,\n",
      "          -2.0378e-01,  4.9585e-02,  9.6908e-02, -1.7717e-01, -1.2460e-01],\n",
      "         [ 2.4725e-01, -4.1160e-02,  3.4064e-01, -3.1884e-03,  1.4066e-01,\n",
      "           2.0515e-01,  1.1264e-02, -2.0344e-01, -5.5762e-02, -1.3623e-02,\n",
      "          -5.0649e-02,  8.2813e-02,  1.2925e-01,  1.5113e-01, -2.1925e-01,\n",
      "           1.1710e-01, -5.6977e-02,  2.0548e-01, -5.1494e-02, -3.1311e-01],\n",
      "         [ 5.9502e-02,  2.4334e-01,  1.4970e-01,  5.3133e-02,  8.1388e-02,\n",
      "          -8.5241e-02, -5.9480e-02,  2.4501e-01,  8.4126e-02,  5.8386e-02,\n",
      "           1.9865e-01, -1.0071e-01, -8.2413e-02, -6.2209e-02,  1.0888e-01,\n",
      "          -2.0413e-01, -1.1478e-01, -1.0215e-02,  5.3805e-02,  1.4786e-02],\n",
      "         [ 1.0261e-01, -2.2047e-01, -1.2762e-01,  1.1327e-01,  1.1780e-01,\n",
      "          -1.5184e-01, -9.5697e-02, -3.4514e-02,  3.6720e-02,  3.2152e-01,\n",
      "          -1.6468e-01,  9.3590e-02,  4.8083e-02,  1.5113e-02, -8.8268e-02,\n",
      "           1.4056e-01,  1.1133e-01,  1.4491e-01,  6.4908e-02,  3.0985e-02],\n",
      "         [ 6.2464e-02,  1.0091e-01, -6.3081e-02, -2.0140e-01,  5.4318e-02,\n",
      "          -4.7916e-02, -7.6179e-02,  3.3642e-01, -1.5883e-01,  1.2249e-01,\n",
      "           1.2429e-01,  2.6807e-01,  1.9682e-01, -1.9200e-01,  1.2766e-01,\n",
      "          -1.2826e-01, -1.0802e-01,  6.5486e-02,  1.4510e-01, -1.6874e-01]],\n",
      "\n",
      "        [[-6.8897e-02, -5.8698e-03, -8.1976e-02,  2.9813e-02, -4.2810e-02,\n",
      "          -3.0602e-01,  6.2268e-02, -3.3499e-02, -3.0379e-02, -2.6225e-01,\n",
      "           1.1783e-01,  4.4598e-03,  2.5564e-01, -2.6996e-01, -3.3267e-02,\n",
      "           6.6353e-03,  6.3648e-02,  5.0881e-02, -2.9494e-01, -1.9652e-01],\n",
      "         [-2.4073e-01,  5.0795e-02,  4.1294e-02, -1.3663e-01, -7.9337e-02,\n",
      "          -1.2418e-01, -3.5172e-01, -2.0520e-01, -1.2001e-01,  8.9993e-02,\n",
      "          -7.0849e-02,  1.4622e-01,  2.9938e-01,  8.4937e-02, -2.0484e-01,\n",
      "           4.8010e-02, -3.9149e-02, -4.2605e-02, -3.0729e-01,  7.3822e-02],\n",
      "         [-1.2854e-01, -8.0142e-02, -2.2541e-01, -2.6028e-02, -1.2705e-01,\n",
      "          -1.4178e-01, -9.5481e-02,  5.7510e-02,  3.1166e-01,  2.0966e-01,\n",
      "          -5.5346e-02, -2.9261e-01, -2.8093e-01, -1.9250e-01, -2.3602e-02,\n",
      "           7.1340e-02,  4.1574e-02, -1.3442e-02,  1.7376e-01,  3.3862e-02],\n",
      "         [ 1.8698e-01, -1.5931e-02,  6.3082e-03,  1.8880e-01, -2.0924e-02,\n",
      "          -3.3067e-02,  9.9809e-02,  1.1922e-01,  1.2181e-01,  3.4696e-02,\n",
      "          -2.4998e-01, -1.1200e-01,  4.3891e-03, -3.0558e-01,  1.4619e-01,\n",
      "          -3.8023e-02, -2.6496e-02,  2.6618e-02, -9.5239e-02, -3.4746e-02],\n",
      "         [ 1.7394e-01, -3.4632e-01,  2.2916e-02, -1.0344e-01,  1.4307e-01,\n",
      "           1.2360e-01,  2.6955e-01, -1.2415e-02, -1.2401e-01, -1.1544e-01,\n",
      "          -1.4574e-01, -9.1810e-02,  3.7854e-02, -9.0850e-02,  5.1038e-02,\n",
      "          -2.2565e-01, -9.9421e-02,  2.2176e-01,  5.2121e-02, -2.1809e-01]],\n",
      "\n",
      "        [[-4.6105e-01,  9.6179e-02, -7.9121e-02, -4.3846e-01,  1.6125e-01,\n",
      "          -2.6929e-01, -9.6205e-02,  1.0404e-02, -9.0446e-02,  1.7858e-01,\n",
      "           4.8552e-02, -2.4062e-01, -3.2198e-01, -2.3714e-02,  4.5355e-02,\n",
      "          -9.0521e-02, -1.3976e-01,  2.2782e-01, -1.2096e-01,  1.4004e-01],\n",
      "         [-4.0673e-01,  6.7947e-02,  6.9390e-04,  2.5586e-01,  5.1808e-02,\n",
      "           1.9286e-01,  4.6938e-02, -1.5119e-01,  1.9164e-01,  6.8240e-02,\n",
      "          -2.0632e-01, -2.9333e-01, -4.2500e-02,  4.6419e-02,  1.1830e-01,\n",
      "          -9.4741e-02, -7.8426e-02, -1.6202e-01, -8.6077e-02,  1.6223e-01],\n",
      "         [-2.1693e-01, -1.2243e-01,  1.1584e-01,  6.1099e-02,  1.0987e-01,\n",
      "           1.3030e-01,  1.2592e-03,  2.2379e-01, -5.9901e-02, -9.0749e-02,\n",
      "          -4.0408e-02,  2.9210e-02, -3.1354e-01,  5.7422e-02,  6.4613e-02,\n",
      "          -1.1294e-01,  2.9691e-01, -2.1082e-01,  5.1165e-02, -1.4727e-01],\n",
      "         [-3.9041e-03,  5.2146e-02,  3.0632e-01, -7.4375e-02, -7.4765e-02,\n",
      "           2.7210e-02, -2.9463e-03, -4.5226e-02, -3.0102e-02, -2.2848e-01,\n",
      "          -9.6563e-02, -9.3807e-02,  9.6019e-03,  1.1474e-02, -5.4547e-02,\n",
      "          -2.3579e-01, -1.5710e-01, -1.5277e-01,  2.8053e-02, -1.5516e-01],\n",
      "         [-1.5317e-01, -1.3500e-01,  6.8736e-02,  2.2474e-01, -1.3062e-02,\n",
      "          -1.1516e-01, -1.4357e-01,  7.4587e-02, -4.5848e-02, -3.2510e-01,\n",
      "          -4.4376e-02,  6.9616e-03,  1.1114e-01,  2.0557e-01, -2.4647e-02,\n",
      "          -5.0910e-02,  3.9269e-02,  2.7499e-01,  1.2493e-01,  7.7609e-02]],\n",
      "\n",
      "        [[ 4.0163e-02, -2.3187e-02, -9.3750e-02,  3.7780e-02,  2.7764e-01,\n",
      "          -6.8466e-02, -1.8021e-01,  8.6623e-02, -1.2706e-01, -8.5767e-02,\n",
      "           2.4498e-01,  5.7671e-02, -1.5479e-01,  1.5704e-02, -2.2439e-01,\n",
      "          -4.3201e-02, -1.0967e-01, -1.8680e-01,  2.7173e-02,  2.2161e-01],\n",
      "         [-2.2431e-01,  2.1595e-02,  3.6797e-02,  5.2983e-02, -1.7432e-02,\n",
      "           1.3900e-01,  1.8030e-01, -8.6496e-02,  4.1868e-01, -2.2959e-01,\n",
      "           7.6063e-02, -2.6946e-01, -3.3079e-02, -2.2178e-01,  4.2883e-02,\n",
      "          -9.4360e-02, -1.1316e-01,  2.3306e-01, -2.5758e-01, -9.5772e-02],\n",
      "         [ 1.0406e-02, -5.3459e-02, -2.2627e-01,  2.5696e-01, -1.7852e-01,\n",
      "          -1.2786e-01,  1.2591e-01, -1.7236e-01,  8.6014e-02,  4.8058e-02,\n",
      "          -2.2345e-01,  1.8952e-01, -1.7058e-01, -2.1725e-01, -2.1903e-02,\n",
      "          -5.0595e-03,  7.4760e-02, -9.6191e-02, -3.1502e-02, -6.0845e-03],\n",
      "         [-4.1385e-02,  1.0030e-01,  6.4643e-02, -1.2993e-02, -1.5482e-01,\n",
      "           5.3988e-02, -4.2492e-02, -2.1691e-02,  2.4591e-01,  5.4671e-02,\n",
      "           2.1756e-01, -4.4594e-02, -1.8481e-01, -1.9126e-03, -1.3565e-01,\n",
      "           1.8346e-01,  1.4258e-02,  2.2121e-02, -4.5059e-03,  1.0640e-01],\n",
      "         [-4.3026e-01, -1.1735e-01, -1.8304e-01, -2.1363e-01,  8.9988e-02,\n",
      "           2.0191e-06, -2.6205e-01,  2.7935e-03, -9.1947e-02, -1.1569e-01,\n",
      "           3.0313e-01, -1.8564e-01,  1.1844e-01, -8.2582e-03, -1.5057e-01,\n",
      "           3.7113e-02, -2.5359e-01, -1.0390e-01, -1.5111e-01,  9.0749e-02]]])\n",
      "2 not perturbed tensor([[[ 0.0801,  0.0167, -0.0585, -0.1862,  0.0491, -0.0075,  0.0407,\n",
      "           0.1108, -0.0391, -0.2614,  0.2509, -0.2170, -0.1122,  0.1285,\n",
      "          -0.0187, -0.2848,  0.0634,  0.0412,  0.2774, -0.2656],\n",
      "         [ 0.2342, -0.0763, -0.1372, -0.2215,  0.2084,  0.0181, -0.0457,\n",
      "           0.1142, -0.2234, -0.0121,  0.1326, -0.0710, -0.2131,  0.1942,\n",
      "           0.1320,  0.0346, -0.2041, -0.0353,  0.0350, -0.0508]],\n",
      "\n",
      "        [[ 0.1051, -0.1419, -0.1796, -0.0498, -0.4159, -0.0174, -0.1365,\n",
      "          -0.0259, -0.1046, -0.1415, -0.0712,  0.0232, -0.0451, -0.0471,\n",
      "          -0.0086,  0.0130,  0.0063,  0.1821, -0.0773, -0.0329],\n",
      "         [-0.0018, -0.0750,  0.0725, -0.2332,  0.0922,  0.2125, -0.0390,\n",
      "          -0.0863, -0.0841, -0.2667, -0.2287,  0.0076,  0.3249, -0.1972,\n",
      "           0.3232, -0.1783, -0.0287, -0.1719, -0.1167, -0.0527]],\n",
      "\n",
      "        [[-0.1529,  0.3289,  0.0148,  0.0165,  0.0763,  0.1056, -0.1037,\n",
      "           0.0511,  0.2008, -0.2814,  0.1716,  0.1851, -0.1418,  0.2242,\n",
      "           0.0393, -0.0651, -0.0253, -0.0445,  0.1025,  0.1122],\n",
      "         [-0.0599, -0.1272, -0.0124,  0.1125,  0.1591,  0.0998,  0.1273,\n",
      "           0.0787,  0.1159, -0.0115, -0.1104,  0.1129, -0.3200, -0.0119,\n",
      "           0.0994, -0.2822,  0.0791, -0.2267, -0.2073,  0.1042]],\n",
      "\n",
      "        [[-0.0450, -0.0416, -0.0523,  0.0558, -0.0159,  0.1683, -0.2287,\n",
      "           0.2432,  0.0769,  0.0041,  0.1973,  0.0368, -0.1018,  0.1834,\n",
      "           0.1709, -0.0245,  0.0248, -0.0808, -0.1257, -0.0588],\n",
      "         [ 0.2028, -0.1417, -0.0331,  0.0714,  0.1003,  0.0204,  0.0310,\n",
      "           0.0764,  0.0236, -0.3010,  0.0537, -0.0052,  0.0158, -0.0909,\n",
      "          -0.0712,  0.1811, -0.0094, -0.1112,  0.1307,  0.0469]],\n",
      "\n",
      "        [[-0.0299,  0.1513,  0.1467, -0.0837,  0.0463,  0.0719, -0.0803,\n",
      "          -0.0812,  0.0136,  0.0077, -0.0364,  0.0618, -0.1139,  0.1178,\n",
      "           0.0344, -0.0166,  0.0462, -0.1675, -0.0498, -0.0292],\n",
      "         [-0.2231, -0.0264, -0.1191, -0.1142, -0.0633, -0.0303,  0.0282,\n",
      "          -0.1609,  0.1888,  0.1468, -0.1237, -0.1932,  0.1596,  0.0827,\n",
      "          -0.3041, -0.0256, -0.1102,  0.0438,  0.0506, -0.2377]],\n",
      "\n",
      "        [[ 0.0877,  0.0153,  0.0925, -0.0696, -0.0691,  0.2079, -0.0986,\n",
      "           0.1354,  0.2073,  0.1057, -0.0712,  0.0561,  0.1082,  0.0951,\n",
      "           0.0913,  0.2029,  0.0312, -0.1308,  0.2581,  0.0651],\n",
      "         [-0.1066,  0.0139,  0.1039,  0.4061, -0.1060,  0.2730,  0.1649,\n",
      "          -0.2912,  0.1781, -0.0839,  0.0926,  0.1212,  0.2443,  0.0465,\n",
      "           0.1172, -0.0822, -0.0094,  0.0760, -0.1776,  0.0482]],\n",
      "\n",
      "        [[ 0.1640,  0.2085,  0.0170,  0.1557,  0.2855,  0.0362,  0.0250,\n",
      "          -0.0672,  0.0188, -0.0375,  0.0547,  0.1391,  0.0628,  0.3520,\n",
      "           0.0805,  0.1091, -0.0751, -0.0245, -0.0872,  0.0271],\n",
      "         [-0.0335, -0.0726,  0.0503,  0.0645,  0.1996,  0.1047,  0.1634,\n",
      "          -0.1549,  0.0510, -0.0582,  0.0015, -0.3621,  0.1720,  0.0538,\n",
      "           0.1421,  0.0118, -0.2486,  0.0105,  0.0500, -0.0926]],\n",
      "\n",
      "        [[ 0.0781,  0.0309,  0.2838,  0.2212,  0.0891,  0.0127,  0.1736,\n",
      "          -0.1164,  0.0098,  0.0406,  0.2618,  0.2337, -0.0484,  0.0804,\n",
      "          -0.0801, -0.0565,  0.1096, -0.2453, -0.0243, -0.0267],\n",
      "         [-0.0026,  0.1176, -0.0672,  0.0990,  0.0838,  0.0340, -0.0085,\n",
      "          -0.0437,  0.3491, -0.0502, -0.1251, -0.1522, -0.0315, -0.2139,\n",
      "           0.0693, -0.0413, -0.0327, -0.0334,  0.1742,  0.1793]]])\n",
      "2 perturbed tensor([[[ 6.9512e-02,  1.5370e-02, -4.4296e-02, -2.0023e-01,  4.3527e-02,\n",
      "          -2.3594e-02,  3.2413e-02,  1.1753e-01, -4.3747e-02, -2.6553e-01,\n",
      "           2.5343e-01, -1.9621e-01, -1.0628e-01,  1.1999e-01, -1.9048e-02,\n",
      "          -2.9430e-01,  5.2543e-02,  4.1828e-02,  2.6850e-01, -2.6557e-01],\n",
      "         [ 2.4939e-01, -7.7373e-02, -1.3123e-01, -2.1044e-01,  2.1342e-01,\n",
      "           3.1010e-02, -4.7450e-02,  1.2894e-01, -2.1910e-01, -1.9565e-02,\n",
      "           1.4623e-01, -5.7346e-02, -2.2578e-01,  1.8804e-01,  1.1878e-01,\n",
      "           2.7442e-02, -1.8689e-01, -4.6157e-02,  3.2078e-02, -5.7265e-02]],\n",
      "\n",
      "        [[ 9.7075e-02, -1.3943e-01, -1.9703e-01, -3.3076e-02, -4.1570e-01,\n",
      "          -3.0560e-02, -1.2949e-01, -7.3024e-03, -9.7953e-02, -1.4893e-01,\n",
      "          -6.3835e-02,  2.9040e-02, -4.4588e-02, -3.8112e-02,  4.5519e-03,\n",
      "           1.3875e-02,  7.7697e-03,  1.9386e-01, -6.7818e-02, -3.2664e-02],\n",
      "         [ 2.3751e-03, -7.7914e-02,  7.3077e-02, -2.3491e-01,  7.5088e-02,\n",
      "           2.1616e-01, -4.5981e-02, -7.6280e-02, -9.3931e-02, -2.6447e-01,\n",
      "          -2.3564e-01, -1.1435e-02,  3.2462e-01, -1.9782e-01,  3.2153e-01,\n",
      "          -1.8442e-01, -2.2934e-02, -1.9215e-01, -1.2114e-01, -5.1393e-02]],\n",
      "\n",
      "        [[-1.5322e-01,  3.2543e-01,  7.6714e-03,  1.9635e-02,  7.0999e-02,\n",
      "           1.1663e-01, -1.0706e-01,  5.7060e-02,  1.8596e-01, -2.8736e-01,\n",
      "           1.6573e-01,  1.7934e-01, -1.6096e-01,  2.2256e-01,  3.8515e-02,\n",
      "          -7.1853e-02, -3.2883e-02, -4.8171e-02,  1.1193e-01,  1.1930e-01],\n",
      "         [-6.5236e-02, -1.2337e-01, -8.1793e-03,  1.1120e-01,  1.5510e-01,\n",
      "           1.0544e-01,  1.0282e-01,  8.6777e-02,  1.0646e-01, -1.9883e-02,\n",
      "          -1.0447e-01,  1.1102e-01, -3.2114e-01, -1.6522e-02,  9.5342e-02,\n",
      "          -2.7627e-01,  8.2051e-02, -2.1767e-01, -2.0294e-01,  1.0633e-01]],\n",
      "\n",
      "        [[-6.6195e-02, -2.5806e-02, -4.4794e-02,  7.2956e-02, -1.5892e-02,\n",
      "           1.6848e-01, -2.2724e-01,  2.5050e-01,  8.3826e-02,  1.3198e-02,\n",
      "           1.8949e-01,  4.1393e-02, -8.7081e-02,  1.9224e-01,  1.7602e-01,\n",
      "          -3.5984e-02,  3.5730e-02, -7.7078e-02, -1.2190e-01, -4.9219e-02],\n",
      "         [ 2.1348e-01, -1.4146e-01, -3.8732e-02,  7.5905e-02,  8.8286e-02,\n",
      "           1.5871e-02,  4.2713e-02,  7.1812e-02,  2.6532e-02, -2.8881e-01,\n",
      "           6.6221e-02, -7.1585e-03,  2.4954e-02, -1.1233e-01, -6.7948e-02,\n",
      "           1.8209e-01, -2.7912e-02, -1.1124e-01,  1.3130e-01,  6.0890e-02]],\n",
      "\n",
      "        [[-3.2193e-02,  1.7547e-01,  1.4165e-01, -9.1144e-02,  4.6158e-02,\n",
      "           7.7900e-02, -6.4318e-02, -7.6133e-02,  9.5817e-03,  2.4230e-04,\n",
      "          -3.3937e-02,  7.6373e-02, -1.1876e-01,  1.1951e-01,  2.8815e-02,\n",
      "          -2.1793e-02,  5.5951e-02, -1.7032e-01, -5.6427e-02, -1.6016e-02],\n",
      "         [-2.2847e-01, -2.7251e-02, -1.3243e-01, -1.1680e-01, -7.9020e-02,\n",
      "          -2.4703e-02,  4.7433e-02, -1.5403e-01,  2.0706e-01,  1.4676e-01,\n",
      "          -1.1588e-01, -1.9655e-01,  1.4465e-01,  8.8213e-02, -3.2238e-01,\n",
      "          -3.7880e-02, -1.0821e-01,  3.5276e-02,  5.6844e-02, -2.3626e-01]],\n",
      "\n",
      "        [[ 9.6616e-02, -2.1341e-03,  8.6905e-02, -5.8493e-02, -6.8031e-02,\n",
      "           2.0194e-01, -1.1066e-01,  1.2925e-01,  1.9359e-01,  1.1132e-01,\n",
      "          -6.5870e-02,  5.3115e-02,  1.0842e-01,  1.0211e-01,  1.0103e-01,\n",
      "           2.0548e-01,  3.8487e-02, -1.1815e-01,  2.6057e-01,  6.9742e-02],\n",
      "         [-1.1836e-01,  1.4493e-02,  1.0012e-01,  4.0239e-01, -1.0118e-01,\n",
      "           2.5857e-01,  1.7916e-01, -2.7933e-01,  1.7811e-01, -7.7051e-02,\n",
      "           8.3343e-02,  1.2331e-01,  2.5333e-01,  4.5151e-02,  1.0493e-01,\n",
      "          -8.1680e-02,  2.6505e-03,  8.5895e-02, -1.8153e-01,  4.5944e-02]],\n",
      "\n",
      "        [[ 1.5556e-01,  2.1721e-01,  1.3465e-02,  1.5722e-01,  2.7908e-01,\n",
      "           4.8867e-02,  1.8787e-02, -6.2858e-02,  1.2849e-02, -3.9210e-02,\n",
      "           4.6681e-02,  1.3381e-01,  5.5337e-02,  3.4492e-01,  7.5415e-02,\n",
      "           1.0113e-01, -8.6672e-02, -3.5112e-02, -1.0934e-01,  2.7355e-02],\n",
      "         [-4.0238e-02, -8.4298e-02,  6.4062e-02,  6.2674e-02,  2.2650e-01,\n",
      "           9.9573e-02,  1.5951e-01, -1.5176e-01,  4.7792e-02, -4.6760e-02,\n",
      "          -4.3621e-04, -3.7264e-01,  1.7402e-01,  3.9698e-02,  1.4830e-01,\n",
      "           3.8039e-03, -2.5610e-01,  8.8290e-03,  3.7070e-02, -1.0666e-01]],\n",
      "\n",
      "        [[ 7.0022e-02,  4.6938e-02,  2.7337e-01,  2.1105e-01,  9.7533e-02,\n",
      "           2.4758e-02,  1.6637e-01, -1.0243e-01,  1.9598e-02,  3.3313e-02,\n",
      "           2.6832e-01,  2.3296e-01, -4.5460e-02,  8.8471e-02, -8.3655e-02,\n",
      "          -6.4169e-02,  1.0571e-01, -2.7228e-01, -2.2931e-02, -3.8018e-02],\n",
      "         [-2.2208e-03,  1.1401e-01, -5.5789e-02,  8.3565e-02,  9.4755e-02,\n",
      "           1.3698e-02, -9.8209e-03, -5.9272e-02,  3.5106e-01, -4.1636e-02,\n",
      "          -1.3138e-01, -1.5688e-01, -5.8307e-02, -2.2241e-01,  8.3433e-02,\n",
      "          -3.9370e-02, -3.0371e-02, -4.1426e-02,  1.5326e-01,  1.8115e-01]]])\n",
      "3 not perturbed tensor([[[ 0.0864]],\n",
      "\n",
      "        [[-0.0135]],\n",
      "\n",
      "        [[ 0.1021]],\n",
      "\n",
      "        [[ 0.2532]]])\n",
      "3 perturbed tensor([[[ 0.0805]],\n",
      "\n",
      "        [[-0.0013]],\n",
      "\n",
      "        [[ 0.1108]],\n",
      "\n",
      "        [[ 0.2427]]])\n"
     ]
    }
   ],
   "source": [
    "from tensor_layers.layers import TensorizedLinear\n",
    "from tensor_layers.low_rank_tensors import CP,TensorTrain,TensorTrainMatrix,Tucker\n",
    "\n",
    "sigma = 0.01\n",
    "\n",
    "for m in model.modules():\n",
    "  # print(type(m))\n",
    "  if isinstance(m, TensorizedLinear):\n",
    "    if isinstance(m.tensor, TensorTrainMatrix):\n",
    "      dims = len(m.tensor.dims[0])\n",
    "      print(dims)\n",
    "      for dim in range(dims):\n",
    "        tt_core = m.tensor.trainable_variables[dim]\n",
    "        print(dim, 'not perturbed', tt_core.data[0])\n",
    "        delta_ = perturb_gaussian(tt_core, sigma)\n",
    "        delta_.to(device)\n",
    "        tt_core = tt_core + delta_\n",
    "        m.tensor.trainable_variables[dim] = nn.Parameter(tt_core)\n",
    "        print(dim, 'perturbed', tt_core.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def perturb_bernoulli(weight, sigma):\n",
    "  mask = 0.5*torch.ones_like(weight)\n",
    "  mask = -sigma*torch.ones_like(weight) + 2*sigma*torch.bernoulli(mask)\n",
    "\n",
    "  return mask\n",
    "\n",
    "def perturb_gaussian(weight, sigma):\n",
    "  w_size = weight.size()\n",
    "  \n",
    "  return torch.normal(0, sigma, size=w_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "6rCl3IMeLXaq",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 batch 0 SGD Train loss is: 2.2955167293548584\n",
      "epoch 0 batch 10 SGD Train loss is: 2.3069891929626465\n",
      "epoch 0 batch 20 SGD Train loss is: 2.2935168743133545\n",
      "epoch 0 batch 30 SGD Train loss is: 2.306501865386963\n",
      "epoch 0 batch 40 SGD Train loss is: 2.2884714603424072\n",
      "epoch 0 batch 50 SGD Train loss is: 2.2873013019561768\n",
      "epoch 0 batch 60 SGD Train loss is: 2.291823387145996\n",
      "epoch 0 batch 70 SGD Train loss is: 2.2894246578216553\n",
      "epoch 0 batch 80 SGD Train loss is: 2.2968289852142334\n",
      "epoch 0 batch 90 SGD Train loss is: 2.26694917678833\n",
      "epoch 0 batch 100 SGD Train loss is: 2.2794787883758545\n",
      "epoch 0 batch 110 SGD Train loss is: 2.2911031246185303\n",
      "epoch 0 batch 120 SGD Train loss is: 2.2747271060943604\n",
      "epoch 0 batch 130 SGD Train loss is: 2.26202654838562\n",
      "epoch 0 batch 140 SGD Train loss is: 2.2768654823303223\n",
      "epoch 0 batch 150 SGD Train loss is: 2.2515969276428223\n",
      "epoch 0 batch 160 SGD Train loss is: 2.2660298347473145\n",
      "epoch 0 batch 170 SGD Train loss is: 2.2645606994628906\n",
      "epoch 0 batch 180 SGD Train loss is: 2.263397693634033\n",
      "epoch 0 batch 190 SGD Train loss is: 2.251437187194824\n",
      "epoch 0 batch 200 SGD Train loss is: 2.2710726261138916\n",
      "epoch 0 batch 210 SGD Train loss is: 2.2576911449432373\n",
      "epoch 0 batch 220 SGD Train loss is: 2.2711074352264404\n",
      "epoch 0 batch 230 SGD Train loss is: 2.242415189743042\n",
      "epoch 0 batch 240 SGD Train loss is: 2.262028217315674\n",
      "epoch 0 batch 250 SGD Train loss is: 2.246415615081787\n",
      "epoch 0 batch 260 SGD Train loss is: 2.254969596862793\n",
      "epoch 0 batch 270 SGD Train loss is: 2.2540509700775146\n",
      "epoch 0 batch 280 SGD Train loss is: 2.232698917388916\n",
      "epoch 0 batch 290 SGD Train loss is: 2.2316064834594727\n",
      "epoch 0 batch 300 SGD Train loss is: 2.23172926902771\n",
      "epoch 0 batch 310 SGD Train loss is: 2.23010516166687\n",
      "epoch 0 batch 320 SGD Train loss is: 2.2448461055755615\n",
      "epoch 0 batch 330 SGD Train loss is: 2.2453484535217285\n",
      "epoch 0 batch 340 SGD Train loss is: 2.207273244857788\n",
      "epoch 0 batch 350 SGD Train loss is: 2.2399308681488037\n",
      "epoch 0 batch 360 SGD Train loss is: 2.235069990158081\n",
      "epoch 0 batch 370 SGD Train loss is: 2.218778371810913\n",
      "epoch 0 batch 380 SGD Train loss is: 2.1983840465545654\n",
      "epoch 0 batch 390 SGD Train loss is: 2.2334301471710205\n",
      "epoch 0 batch 400 SGD Train loss is: 2.205747365951538\n",
      "epoch 0 batch 410 SGD Train loss is: 2.191236734390259\n",
      "epoch 0 batch 420 SGD Train loss is: 2.2229220867156982\n",
      "epoch 0 batch 430 SGD Train loss is: 2.188235282897949\n",
      "epoch 0 batch 440 SGD Train loss is: 2.2099609375\n",
      "epoch 0 batch 450 SGD Train loss is: 2.1989798545837402\n",
      "epoch 0 batch 460 SGD Train loss is: 2.167789936065674\n",
      "Epoch 0 complete! Average Training loss: 2.250947621331287\n",
      "Validation loss is: 2.191731377492977\n",
      "Accuracy score is 0.44264240506329117\n",
      "F1-score is 0.3802218512827177\n",
      "epoch 1 batch 0 SGD Train loss is: 2.1683783531188965\n",
      "epoch 1 batch 10 SGD Train loss is: 2.187068462371826\n",
      "epoch 1 batch 20 SGD Train loss is: 2.1736207008361816\n",
      "epoch 1 batch 30 SGD Train loss is: 2.2035930156707764\n",
      "epoch 1 batch 40 SGD Train loss is: 2.176257371902466\n",
      "epoch 1 batch 50 SGD Train loss is: 2.1756553649902344\n",
      "epoch 1 batch 60 SGD Train loss is: 2.2054309844970703\n",
      "epoch 1 batch 70 SGD Train loss is: 2.159893274307251\n",
      "epoch 1 batch 80 SGD Train loss is: 2.152360200881958\n",
      "epoch 1 batch 90 SGD Train loss is: 2.17763614654541\n",
      "epoch 1 batch 100 SGD Train loss is: 2.18314266204834\n",
      "epoch 1 batch 110 SGD Train loss is: 2.197309970855713\n",
      "epoch 1 batch 120 SGD Train loss is: 2.1900575160980225\n",
      "epoch 1 batch 130 SGD Train loss is: 2.172091484069824\n",
      "epoch 1 batch 140 SGD Train loss is: 2.1678929328918457\n",
      "epoch 1 batch 150 SGD Train loss is: 2.134697198867798\n",
      "epoch 1 batch 160 SGD Train loss is: 2.1645596027374268\n",
      "epoch 1 batch 170 SGD Train loss is: 2.132829189300537\n",
      "epoch 1 batch 180 SGD Train loss is: 2.1352083683013916\n",
      "epoch 1 batch 190 SGD Train loss is: 2.135472297668457\n",
      "epoch 1 batch 200 SGD Train loss is: 2.1322195529937744\n",
      "epoch 1 batch 210 SGD Train loss is: 2.1428935527801514\n",
      "epoch 1 batch 220 SGD Train loss is: 2.146794080734253\n",
      "epoch 1 batch 230 SGD Train loss is: 2.11564040184021\n",
      "epoch 1 batch 240 SGD Train loss is: 2.1506776809692383\n",
      "epoch 1 batch 250 SGD Train loss is: 2.1343436241149902\n",
      "epoch 1 batch 260 SGD Train loss is: 2.1375572681427\n",
      "epoch 1 batch 270 SGD Train loss is: 2.12115740776062\n",
      "epoch 1 batch 280 SGD Train loss is: 2.1423583030700684\n",
      "epoch 1 batch 290 SGD Train loss is: 2.102715015411377\n",
      "epoch 1 batch 300 SGD Train loss is: 2.119187116622925\n",
      "epoch 1 batch 310 SGD Train loss is: 2.14959454536438\n",
      "epoch 1 batch 320 SGD Train loss is: 2.112682580947876\n",
      "epoch 1 batch 330 SGD Train loss is: 2.147942543029785\n",
      "epoch 1 batch 340 SGD Train loss is: 2.089902877807617\n",
      "epoch 1 batch 350 SGD Train loss is: 2.1328930854797363\n",
      "epoch 1 batch 360 SGD Train loss is: 2.1347744464874268\n",
      "epoch 1 batch 370 SGD Train loss is: 2.1021218299865723\n",
      "epoch 1 batch 380 SGD Train loss is: 2.0515899658203125\n",
      "epoch 1 batch 390 SGD Train loss is: 2.123197317123413\n",
      "epoch 1 batch 400 SGD Train loss is: 2.0857250690460205\n",
      "epoch 1 batch 410 SGD Train loss is: 2.0599894523620605\n",
      "epoch 1 batch 420 SGD Train loss is: 2.093210458755493\n",
      "epoch 1 batch 430 SGD Train loss is: 2.0553267002105713\n",
      "epoch 1 batch 440 SGD Train loss is: 2.1002719402313232\n",
      "epoch 1 batch 450 SGD Train loss is: 2.0788543224334717\n",
      "epoch 1 batch 460 SGD Train loss is: 2.0176312923431396\n",
      "Epoch 1 complete! Average Training loss: 2.140416206327327\n",
      "Validation loss is: 2.071804272977612\n",
      "Accuracy score is 0.6207476265822784\n",
      "F1-score is 0.5834381269504976\n",
      "epoch 2 batch 0 SGD Train loss is: 2.050442695617676\n",
      "epoch 2 batch 10 SGD Train loss is: 2.0485386848449707\n",
      "epoch 2 batch 20 SGD Train loss is: 2.037001848220825\n",
      "epoch 2 batch 30 SGD Train loss is: 2.076672315597534\n",
      "epoch 2 batch 40 SGD Train loss is: 2.052082061767578\n",
      "epoch 2 batch 50 SGD Train loss is: 2.06288480758667\n",
      "epoch 2 batch 60 SGD Train loss is: 2.0897669792175293\n",
      "epoch 2 batch 70 SGD Train loss is: 2.0230979919433594\n",
      "epoch 2 batch 80 SGD Train loss is: 2.0323081016540527\n",
      "epoch 2 batch 90 SGD Train loss is: 2.0523152351379395\n",
      "epoch 2 batch 100 SGD Train loss is: 2.0581047534942627\n",
      "epoch 2 batch 110 SGD Train loss is: 2.045288562774658\n",
      "epoch 2 batch 120 SGD Train loss is: 2.085391044616699\n",
      "epoch 2 batch 130 SGD Train loss is: 2.067560911178589\n",
      "epoch 2 batch 140 SGD Train loss is: 2.0225281715393066\n",
      "epoch 2 batch 150 SGD Train loss is: 2.016159772872925\n",
      "epoch 2 batch 160 SGD Train loss is: 2.0315496921539307\n",
      "epoch 2 batch 170 SGD Train loss is: 1.980324625968933\n",
      "epoch 2 batch 180 SGD Train loss is: 2.0212607383728027\n",
      "epoch 2 batch 190 SGD Train loss is: 1.9940519332885742\n",
      "epoch 2 batch 200 SGD Train loss is: 2.002915620803833\n",
      "epoch 2 batch 210 SGD Train loss is: 2.006477117538452\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_80988/444352110.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     54\u001b[0m             \u001b[0mweight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m             \u001b[1;31m# print('model', weight[0,0])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m             \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelta_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mperturb_gaussian\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m             \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelta_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelta_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m             \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mParameter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelta_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_80988/3657626686.py\u001b[0m in \u001b[0;36mperturb_gaussian\u001b[1;34m(weight, sigma)\u001b[0m\n\u001b[0;32m     10\u001b[0m   \u001b[0mw_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mw_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run = 1\n",
    "import math\n",
    "import copy\n",
    "pi = torch.tensor(math.pi)\n",
    "sigma = 0.01\n",
    "K = 8\n",
    "model = Net().to(device)\n",
    "\n",
    "if cuda == 1:\n",
    "  model = model.cuda()\n",
    "  DTYPE = torch.cuda.FloatTensor\n",
    "  LONG = torch.cuda.LongTensor\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(reduction='mean')   # output the average loss\n",
    "\n",
    "comment = f'run = {run}'\n",
    "writer = SummaryWriter(comment=comment, log_dir='./tt_runs')\n",
    "\n",
    "# X, y = next(iter(train_dataloader))\n",
    "# X, y = X.to(device), y.to(device)\n",
    "\n",
    "for e in range(epochs):\n",
    "  ############################## TRAIN ###############################\n",
    "  train_len = len(train_dataloader.dataset)\n",
    "  train_batch = len(train_dataloader)\n",
    "\n",
    "  test_len = len(test_dataloader.dataset)\n",
    "  test_batch = len(test_dataloader)\n",
    "  model.train()\n",
    "  model.zero_grad()\n",
    "  avg_train_loss = 0.0\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for batch, (X, y) in enumerate(train_dataloader):\n",
    "      X, y = X.to(device), y.to(device)\n",
    "      \n",
    "      # Forward Propagation\n",
    "      pred = model(X)\n",
    "      loss = criterion(pred, y)\n",
    "      avg_train_loss += loss.data.item() / train_batch\n",
    "\n",
    "      if batch % 10 == 0:\n",
    "        print(\"epoch {} batch {} SGD Train loss is: {}\".format(e, batch, loss.data.item()))\n",
    "        writer.add_scalar(\"SGD Train loss\", loss.data.item(), batch)\n",
    "\n",
    "      # Backward Propagation\n",
    "      # estimate gradient\n",
    "      G = []\n",
    "      for k in range(K):\n",
    "        #f(x+delta_k)\n",
    "        g_model=copy.deepcopy(model)\n",
    "        for m in g_model.modules():\n",
    "          if isinstance(m, nn.Linear):\n",
    "            weight = m.weight.data\n",
    "            # print('model', weight[0,0])\n",
    "            m.delta_ = perturb_gaussian(weight, sigma)\n",
    "            m.delta_ = m.delta_.to(device)\n",
    "            m.weight = nn.Parameter(weight + m.delta_)\n",
    "            # print('x+delta_k',m.weight.data[0,0])\n",
    "          \n",
    "        \n",
    "        pred_1 = g_model(X)\n",
    "        loss_1 = criterion(pred_1, y)\n",
    "\n",
    "        #f(x-delta_k)\n",
    "        # use the same g_model to copy its m.delta_\n",
    "        for m in g_model.modules():\n",
    "          if isinstance(m, nn.Linear):\n",
    "            weight = m.weight.data\n",
    "            # print('model',weight[0,0])\n",
    "            m.weight = nn.Parameter(weight - 2 * m.delta_)\n",
    "            # print('x-delta_k',m.weight.data[0,0])\n",
    "        \n",
    "        pred_2 = g_model(X)\n",
    "        loss_2 = criterion(pred_2, y)\n",
    "\n",
    "        # accumulate gradients\n",
    "        for m in g_model.modules():\n",
    "          if isinstance(m, nn.Linear):\n",
    "            if k == 0:\n",
    "              g = (loss_1-loss_2)/(2*sigma**2)/K * m.delta_ \n",
    "              G.append(g)\n",
    "            else:\n",
    "              g = G.pop(0)\n",
    "              g = g + (loss_1-loss_2)/(2*sigma**2)/K * m.delta_\n",
    "              G.append(g)\n",
    "      # estimate grad end\n",
    "\n",
    "      # SGD\n",
    "      for m in model.modules():\n",
    "        if isinstance(m, nn.Linear):\n",
    "          m.weight = nn.Parameter(m.weight - lr*G.pop(0))\n",
    "\n",
    "    # len(training_data) = len(train_dataloader.dataset) -- how many samples\n",
    "    # len(train_dataloader) -- how many batches\n",
    "    # avg_train_loss = avg_train_loss / len(training_data)  # average over whole training dataset\n",
    "    print(\"Epoch {} complete! Average Training loss: {}\".format(e, avg_train_loss))\n",
    "    writer.add_scalar(\"Avg Train Loss\", avg_train_loss, e)\n",
    "\n",
    "    # Terminate the training process if run into NaN\n",
    "    if np.isnan(avg_train_loss):\n",
    "        print(\"Training got into NaN values...\\n\\n\")\n",
    "        complete = False\n",
    "        break\n",
    "    \n",
    "    ############################## VALID/TEST ###############################\n",
    "    model.eval()\n",
    "    test_loss, acc_score, f1 = 0, 0, 0\n",
    "    avg_valid_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "      for X, y in test_dataloader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        pred = model(X)\n",
    "        avg_valid_loss += criterion(pred, y).item()\n",
    "        \n",
    "        # y itself is true label\n",
    "        all_true_label = y.cpu().data\n",
    "        all_predicted_label = pred.argmax(1).cpu().data\n",
    "        acc_score += accuracy_score(all_true_label, all_predicted_label)\n",
    "        f1 += f1_score(all_true_label, all_predicted_label, average='weighted')\n",
    "\n",
    "        # argmax will eliminate this dimension!\n",
    "        # correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    # in every iteration, the loss of the batch is summed up. actual loss should divide dataset size\n",
    "    avg_valid_loss = avg_valid_loss / test_batch  # average over whole validation dataset\n",
    "    print(\"Validation loss is: {}\".format(avg_valid_loss))\n",
    "    writer.add_scalar(\"Avg Valid Loss\", avg_valid_loss, e)\n",
    "\n",
    "    if np.isnan(avg_valid_loss):\n",
    "      print(\"Training got into NaN values...\\n\\n\")\n",
    "      complete = False\n",
    "      break\n",
    "\n",
    "    # score is calculated every batch\n",
    "    acc_score /= test_batch\n",
    "    f1 /= test_batch\n",
    "\n",
    "    print(\"Accuracy score is {}\".format(acc_score))\n",
    "    print(\"F1-score is {}\".format(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "  Bernoulli perturbtion\n",
    "'''\n",
    "\n",
    "# with torch.no_grad():\n",
    "#   for m in model.modules():\n",
    "#     if isinstance(m, nn.Linear):\n",
    "#       weight = m.weight.data\n",
    "#       print(weight[0,0])\n",
    "#       m.weight.data = weight + perturb_bernoulli(weight, pi)\n",
    "#       print(m.weight.data[0,0])\n",
    "\n",
    "# for param in model.parameters():\n",
    "#   print(param.size(), param[0,0])\n",
    "\n",
    "# for param in g_model.parameters():\n",
    "#   print(param.size(), param[0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kKRziaeFLXar"
   },
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f0xVRtDDbrIE"
   },
   "outputs": [],
   "source": [
    "signiture = 'MNIST'\n",
    "output_dim = 10\n",
    "\n",
    "max_rank = 2\n",
    "max_rank_list = [2,4,8,10,20]\n",
    "max_rank_list = [4]\n",
    "run = 1\n",
    "# runs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vKJRnvRDLXar"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temp location for models: ./tt_models/model_MNIST_4_1.pt\n",
      "Epoch 0 complete! Average Training loss: 0.9904296639283499\n",
      "Validation loss is: 0.36663719880580903\n",
      "Accuracy score is 0.8942078025477707\n",
      "F1-score is 0.8933782245865438\n",
      "Found new best model, saving to disk...\n",
      "Epoch 1 complete! Average Training loss: 0.534040156976382\n",
      "Validation loss is: 0.2880606528520584\n",
      "Accuracy score is 0.916202229299363\n",
      "F1-score is 0.9154574772541343\n",
      "Found new best model, saving to disk...\n",
      "Epoch 2 complete! Average Training loss: 0.4508840109586716\n",
      "Validation loss is: 0.24896067473888397\n",
      "Accuracy score is 0.9270501592356688\n",
      "F1-score is 0.9262492608723443\n",
      "Found new best model, saving to disk...\n",
      "Epoch 3 complete! Average Training loss: 0.40093127624193825\n",
      "Validation loss is: 0.2261153151035309\n",
      "Accuracy score is 0.9336186305732485\n",
      "F1-score is 0.9330336514699196\n",
      "Found new best model, saving to disk...\n",
      "Epoch 4 complete! Average Training loss: 0.3636346802512805\n",
      "Validation loss is: 0.20610321423113345\n",
      "Accuracy score is 0.9389928343949044\n",
      "F1-score is 0.9386421898612806\n",
      "Found new best model, saving to disk...\n",
      "Epoch 5 complete! Average Training loss: 0.3382686674316724\n",
      "Validation loss is: 0.18965016030967236\n",
      "Accuracy score is 0.9444665605095541\n",
      "F1-score is 0.944157019583982\n",
      "Found new best model, saving to disk...\n",
      "Epoch 6 complete! Average Training loss: 0.3158399274905523\n",
      "Validation loss is: 0.17964801611602307\n",
      "Accuracy score is 0.9469546178343949\n",
      "F1-score is 0.9467195304204766\n",
      "Found new best model, saving to disk...\n",
      "Epoch 7 complete! Average Training loss: 0.3032412446300189\n",
      "Validation loss is: 0.17120505470037461\n",
      "Accuracy score is 0.947452229299363\n",
      "F1-score is 0.9471215760179527\n",
      "Found new best model, saving to disk...\n",
      "Epoch 8 complete! Average Training loss: 0.28905540478626884\n",
      "Validation loss is: 0.165959673422575\n",
      "Accuracy score is 0.948546974522293\n",
      "F1-score is 0.9481783254092697\n",
      "Found new best model, saving to disk...\n",
      "Epoch 9 complete! Average Training loss: 0.28206561685403186\n",
      "Validation loss is: 0.15950749341249465\n",
      "Accuracy score is 0.9523288216560509\n",
      "F1-score is 0.9519383123508302\n",
      "Found new best model, saving to disk...\n"
     ]
    }
   ],
   "source": [
    "for max_rank in max_rank_list:\n",
    "#  for run in range(runs):\n",
    "\n",
    "  model_path = os.path.join(model_path_dir, \"model_{}_{}_{}.pt\".format(signiture,max_rank,run))\n",
    "  print(\"Temp location for models: {}\".format(model_path))\n",
    "  os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
    "  best_valid_acc = 0\n",
    "  best_valid_loss = float('inf')\n",
    "  model = OrthoTONN(tensor_type='TensorTrainMatrix',max_rank=max_rank,dropouts=0.5,prior_type='log_uniform',eta=1.0, device=device,dtype=dtype)\n",
    "  if cuda == 1:\n",
    "      model = model.cuda()\n",
    "      DTYPE = torch.cuda.FloatTensor\n",
    "      LONG = torch.cuda.LongTensor\n",
    "\n",
    "  # criterion = nn.CrossEntropyLoss(reduction='sum') \n",
    "  # factors = list(model.parameters())[:3]\n",
    "  # other = list(model.parameters())[3:]\n",
    "  # optimizer = optim.Adam([{\"params\": factors, \"lr\": factor_lr}, {\"params\": other, \"lr\": lr}], weight_decay=decay)\n",
    "\n",
    "  criterion = nn.CrossEntropyLoss(reduction='sum') \n",
    "  optimizer = optim.Adam(list(model.parameters()), lr, weight_decay=decay)\n",
    "  scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5)    # 3\n",
    "\n",
    "  comment = f'rank = {max_rank} run = {run}'\n",
    "  writer = SummaryWriter(comment=comment, log_dir='./tt_runs')\n",
    "\n",
    "  for e in range(epochs):\n",
    "    # TRAIN\n",
    "    size = len(train_dataloader.dataset)\n",
    "    model.train()\n",
    "    model.zero_grad()\n",
    "    avg_train_loss = 0.0\n",
    "\n",
    "    for batch, (X, y) in enumerate(train_dataloader):\n",
    "      X, y = X.to(device), y.to(device)\n",
    "\n",
    "      # Compute prediction error\n",
    "      pred = model(X)\n",
    "      loss = criterion(pred, y)\n",
    "\n",
    "      # Backpropagation\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      avg_train_loss += loss.data.item()\n",
    "      optimizer.step()\n",
    "    \n",
    "    # len(training_data) = len(train_dataloader.dataset) -- how many samples\n",
    "    # len(train_dataloader) -- how many batches\n",
    "    avg_train_loss = avg_train_loss / len(training_data)  # average over whole training dataset\n",
    "    print(\"Epoch {} complete! Average Training loss: {}\".format(e, avg_train_loss))\n",
    "    writer.add_scalar(\"Avg Train Loss\", avg_train_loss, e)\n",
    "\n",
    "    # Terminate the training process if run into NaN\n",
    "    if np.isnan(avg_train_loss):\n",
    "        print(\"Training got into NaN values...\\n\\n\")\n",
    "        complete = False\n",
    "        break\n",
    "    \n",
    "    #Valid/Test\n",
    "    size = len(test_dataloader.dataset)\n",
    "    num_batches = len(test_dataloader)\n",
    "    model.eval()\n",
    "    test_loss, acc_score, f1 = 0, 0, 0\n",
    "    avg_valid_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "      for X, y in test_dataloader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        pred = model(X)\n",
    "        avg_valid_loss += criterion(pred, y).item()\n",
    "        \n",
    "        # y itself is true label\n",
    "        all_true_label = y.cpu().data\n",
    "        all_predicted_label = pred.argmax(1).cpu().data\n",
    "        acc_score += accuracy_score(all_true_label, all_predicted_label)\n",
    "        f1 += f1_score(all_true_label, all_predicted_label, average='weighted')\n",
    "\n",
    "        # argmax will eliminate this dimension!\n",
    "        # correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    # in every iteration, the loss of the batch is summed up. actual loss should divide dataset size\n",
    "    avg_valid_loss = avg_valid_loss / len(test_data)  # average over whole validation dataset\n",
    "    print(\"Validation loss is: {}\".format(avg_valid_loss))\n",
    "    writer.add_scalar(\"Avg Valid Loss\", avg_valid_loss, e)\n",
    "\n",
    "    if np.isnan(avg_valid_loss):\n",
    "      print(\"Training got into NaN values...\\n\\n\")\n",
    "      complete = False\n",
    "      break\n",
    "\n",
    "    # score is calculated every batch\n",
    "    acc_score /= num_batches\n",
    "    f1 /= num_batches\n",
    "\n",
    "    print(\"Accuracy score is {}\".format(acc_score))\n",
    "    print(\"F1-score is {}\".format(f1))\n",
    "    \n",
    "    if (avg_valid_loss < best_valid_loss):\n",
    "        curr_patience = patience\n",
    "        best_valid_loss = avg_valid_loss\n",
    "        torch.save(model, model_path)\n",
    "        print(\"Found new best model, saving to disk...\")\n",
    "    else:\n",
    "        curr_patience -= 1\n",
    "    \n",
    "    if curr_patience <= 0:\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SJZ71IPsF-2V"
   },
   "outputs": [],
   "source": [
    "X = torch.reshape(X,[64,1,28,28])\n",
    "model.eval()\n",
    "X, y = X.to(device), y.to(device)\n",
    "pred = model(X)\n",
    "pred.size(), y.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e88ucjmjMvGK"
   },
   "outputs": [],
   "source": [
    "all_true_label = y.cpu().data\n",
    "all_predicted_label = pred.argmax(1).cpu().data\n",
    "print(\"all_true_label is: {}\".format(all_true_label))\n",
    "print(\"all_predicted_label is: {}\".format(all_predicted_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lgVd2lUxGUp6"
   },
   "outputs": [],
   "source": [
    "pred.argmax(1).cpu().data.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dkuRLICBLXat"
   },
   "source": [
    "## QR decomp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7yIlCIiWPXjE"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6084/2956079895.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmax_rank\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mrun\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmodel_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_path_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"model_{}_{}_{}.pt\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msigniture\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_rank\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mbest_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mbest_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "max_rank = 2\n",
    "run = 1\n",
    "model_path = os.path.join(model_path_dir, \"model_{}_{}_{}.pt\".format(signiture,max_rank,run))\n",
    "best_model = torch.load(model_path)\n",
    "best_model.eval()\n",
    "best_model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 378,
     "status": "ok",
     "timestamp": 1671404507192,
     "user": {
      "displayName": "赵业权",
      "userId": "15926883842834523025"
     },
     "user_tz": 480
    },
    "id": "vyof9bLlXc3_",
    "outputId": "a3cc5299-35f6-4565-fd16-e88d9f9a5c1f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 7, 8, 2])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.fc1.tensor.trainable_variables[2].data.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "executionInfo": {
     "elapsed": 330,
     "status": "error",
     "timestamp": 1671404175264,
     "user": {
      "displayName": "赵业权",
      "userId": "15926883842834523025"
     },
     "user_tz": 480
    },
    "id": "R8bsRy0hawQH",
    "outputId": "eddf3765-be89-45db-f9e3-72cf197b2bb4"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m tt_matrix \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mreshape(tt_core, (size[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m*\u001b[39msize[\u001b[38;5;241m1\u001b[39m], size[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m*\u001b[39msize[\u001b[38;5;241m3\u001b[39m]))\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i_order \u001b[38;5;241m!=\u001b[39m order\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 10\u001b[0m   tt_core \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtt_matrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mL\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m tt_matrix \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtranspose(tt_matrix,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     13\u001b[0m QT, R \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mqr(tt_matrix, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomplete\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (14x16 and 8x4)"
     ]
    }
   ],
   "source": [
    "order = best_model.fc1.tensor.order\n",
    "ortho_cores = []\n",
    "\n",
    "for i_order in range(order-1,-1,-1):\n",
    "\n",
    "  tt_core = best_model.fc1.tensor.trainable_variables[i_order].data\n",
    "  size = tt_core.size()\n",
    "  tt_matrix = torch.reshape(tt_core, (size[0]*size[1], size[2]*size[3]))\n",
    "  if i_order != order-1:\n",
    "    tt_core = torch.matmul(tt_matrix, L)\n",
    "  \n",
    "  tt_matrix = torch.transpose(tt_matrix,0,1)\n",
    "  QT, R = torch.linalg.qr(tt_matrix, mode=\"complete\")\n",
    "  L = torch.transpose(R,0,1)\n",
    "  Q = torch.transpose(Q,0,1)\n",
    "  ortho_cores.insert(0,Q)\n",
    "\n",
    "U, S, Vh = torch.linalg.svd(L)\n",
    "ortho_cores.insert(0,Vh)\n",
    "ortho_cores.insert(0,U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 350,
     "status": "ok",
     "timestamp": 1671221552065,
     "user": {
      "displayName": "赵业权",
      "userId": "15926883842834523025"
     },
     "user_tz": 480
    },
    "id": "tR8tzr51hF3s",
    "outputId": "e547deb1-dd48-46fe-d108-e5f3023b22e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4])\n",
      "tensor([[-0.4715,  0.1050,  0.1914, -0.3383,  0.0488,  0.5277, -0.5783,  0.0174],\n",
      "        [-0.2078, -0.1173,  0.4082,  0.5203,  0.2509, -0.4624, -0.4283, -0.2135],\n",
      "        [-0.0342,  0.0886, -0.3271, -0.5492,  0.2030, -0.6334, -0.2971,  0.2273],\n",
      "        [-0.1245,  0.2322,  0.6431, -0.4141, -0.3474, -0.2737,  0.3126, -0.2286]])\n",
      "tensor([[-0.6149, -0.4502, -0.5039, -0.5871],\n",
      "        [ 0.0000,  0.1646,  0.1201,  0.2936],\n",
      "        [ 0.0000,  0.0000, -0.0480, -0.0273],\n",
      "        [ 0.0000,  0.0000,  0.0000, -0.1658]])\n"
     ]
    }
   ],
   "source": [
    "print(tt_matrix.size())\n",
    "print(Q)\n",
    "print(R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 346,
     "status": "ok",
     "timestamp": 1671219876597,
     "user": {
      "displayName": "赵业权",
      "userId": "15926883842834523025"
     },
     "user_tz": 480
    },
    "id": "_g3ndh6Pa-5v",
    "outputId": "89356ee7-5b06-4eff-cdbd-eb9f46e6a82c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "2\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "from .interferometer import interferometer as itf \n",
    "Q = ortho_cores[-1]\n",
    "I = itf.square_decomposition(Q)\n",
    "I.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrthoTONN(\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (fc1): TensorizedLinear(\n",
       "    in_features=784, out_features=1024, bias=False\n",
       "    (tensor): TensorTrainMatrix(\n",
       "      (trainable_variables): ParameterList(\n",
       "          (0): Parameter containing: [torch.FloatTensor of size 1x4x4x2]\n",
       "          (1): Parameter containing: [torch.FloatTensor of size 2x7x8x2]\n",
       "          (2): Parameter containing: [torch.FloatTensor of size 2x7x8x2]\n",
       "          (3): Parameter containing: [torch.FloatTensor of size 2x4x4x1]\n",
       "          (4): Parameter containing: [torch.FloatTensor of size 1x4x4x2]\n",
       "          (5): Parameter containing: [torch.FloatTensor of size 2x7x8x2]\n",
       "          (6): Parameter containing: [torch.FloatTensor of size 2x7x8x2]\n",
       "          (7): Parameter containing: [torch.FloatTensor of size 2x4x4x1]\n",
       "          (8): Parameter containing: [torch.FloatTensor of size 2]\n",
       "          (9): Parameter containing: [torch.FloatTensor of size 2]\n",
       "          (10): Parameter containing: [torch.FloatTensor of size 2]\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc2): TensorizedLinear(\n",
       "    in_features=1024, out_features=10, bias=False\n",
       "    (tensor): TensorTrainMatrix(\n",
       "      (trainable_variables): ParameterList(\n",
       "          (0): Parameter containing: [torch.FloatTensor of size 1x4x1x2]\n",
       "          (1): Parameter containing: [torch.FloatTensor of size 2x8x5x2]\n",
       "          (2): Parameter containing: [torch.FloatTensor of size 2x8x2x2]\n",
       "          (3): Parameter containing: [torch.FloatTensor of size 2x4x1x1]\n",
       "          (4): Parameter containing: [torch.FloatTensor of size 1x4x1x2]\n",
       "          (5): Parameter containing: [torch.FloatTensor of size 2x8x5x2]\n",
       "          (6): Parameter containing: [torch.FloatTensor of size 2x8x2x2]\n",
       "          (7): Parameter containing: [torch.FloatTensor of size 2x4x1x1]\n",
       "          (8): Parameter containing: [torch.FloatTensor of size 2]\n",
       "          (9): Parameter containing: [torch.FloatTensor of size 2]\n",
       "          (10): Parameter containing: [torch.FloatTensor of size 2]\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_rank = 2\n",
    "run = 1\n",
    "model_path = os.path.join(model_path_dir, \"model_{}_{}_{}.pt\".format(signiture,max_rank,run))\n",
    "best_model = torch.load(model_path)\n",
    "best_model.eval()\n",
    "best_model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import interferometer as itf\n",
    "\n",
    "tt_core = best_model.fc1.tensor.trainable_variables[3].data\n",
    "size = tt_core.size()\n",
    "tt_matrix = torch.reshape(tt_core, (size[0]*size[1], size[2]*size[3]))\n",
    "\n",
    "U, S, Vh = torch.linalg.svd(tt_matrix)\n",
    "# inf requires numpy array\n",
    "U = U.numpy()\n",
    "I = itf.square_decomposition(U)\n",
    "I.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U = tt_matrix.numpy()\n",
    "U2 = np.power(U,2)\n",
    "U2_row = np.sum(U2,axis=0)\n",
    "U2_col = np.sum(U2,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.6180354, 1.0419817, 1.0265269, 2.5757833], dtype=float32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U2_row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YRyJNmLZLXat"
   },
   "outputs": [],
   "source": [
    "def eval_at_model_path(model_path, test_iterator):\n",
    "  best_model = torch.load(model_path)\n",
    "  best_model.eval()\n",
    "  best_model.to(device)\n",
    "\n",
    "  #Valid/Test\n",
    "  size = len(test_iterator.dataset)\n",
    "  num_batches = len(test_iterator)\n",
    "  test_loss, acc_score, f1 = 0, 0, 0\n",
    "  avg_valid_loss = 0.0\n",
    "  with torch.no_grad():\n",
    "    for X, y in test_iterator:\n",
    "      X, y = X.to(device), y.to(device)\n",
    "      pred = best_model(X)\n",
    "      \n",
    "      # y itself is true label\n",
    "      all_true_label = y.cpu().data\n",
    "      all_predicted_label = pred.argmax(1).cpu().data\n",
    "      acc_score += accuracy_score(all_true_label, all_predicted_label)\n",
    "      f1 += f1_score(all_true_label, all_predicted_label, average='weighted')\n",
    "\n",
    "  # score is calculated every batch\n",
    "  acc_score /= num_batches\n",
    "  f1 /= num_batches\n",
    "  print(\"Accuracy score is {}\".format(acc_score))\n",
    "  print(\"F1-score is {}\".format(f1))\n",
    "\n",
    "  return acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4320,
     "status": "ok",
     "timestamp": 1671222715616,
     "user": {
      "displayName": "赵业权",
      "userId": "15926883842834523025"
     },
     "user_tz": 480
    },
    "id": "pc1f5LMXLXau",
    "outputId": "ee063b27-9bc3-43f5-da6c-54b6be8c657f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./tt_models/model_MNIST_2_1.pt\n",
      "Accuracy score is 0.9357085987261147\n",
      "F1-score is 0.9351699481399507\n",
      "-------------\n",
      "./tt_models/model_MNIST_4_1.pt\n",
      "Accuracy score is 0.962281050955414\n",
      "F1-score is 0.9619681106624476\n",
      "-------------\n",
      "./tt_models/model_MNIST_8_1.pt\n",
      "Accuracy score is 0.9735270700636943\n",
      "F1-score is 0.9733664613900941\n",
      "-------------\n"
     ]
    }
   ],
   "source": [
    "best_model_path = 0\n",
    "best_acc = 0\n",
    "for max_rank in max_rank_list:\n",
    "#  for run in range(runs):\n",
    "  model_path = os.path.join(model_path_dir, \"model_{}_{}_{}.pt\".format(signiture,max_rank,run))\n",
    "  print(model_path)\n",
    "  acc = eval_at_model_path(model_path, test_dataloader)\n",
    "  print(\"-------------\")\n",
    "  if acc > best_acc:\n",
    "      best_acc = acc\n",
    "      best_model_path = model_path"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [
    {
     "file_id": "https://gist.github.com/ngrislain/c3ba6f687c64ce31adc6b0dff1b26d6a#file-py38-success-ipynb",
     "timestamp": 1666743016140
    }
   ]
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "pytorch_38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 251.85199999999998,
   "position": {
    "height": "40px",
    "left": "867px",
    "right": "20px",
    "top": "111px",
    "width": "343px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "17010e9482637207e2881aa54cd78fa6a842c0515d16e99f693c5a499c225640"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
